import torch
from torch import nn
from d2l import torch as d2l
import os
import re
import datetime
import time
from typing import Optional, Dict, Any

from src.utils.visualization import VisualizationTool
from src.utils.file_utils import FileUtils
from src.utils.model_registry import ModelRegistry
from src.utils.data_utils import DataLoader
from src.predictor.predictor import Predictor
from src.utils.log_utils import get_logger

# åˆå§‹åŒ–æ—¥å¿—
logger = get_logger(
    name="trainer",
    log_file="logs/trainer.log",
    global_level="DEBUG",
    console_level="INFO",
    file_level="DEBUG"
)

class Trainer:
    """ä¼˜åŒ–åçš„æ¨¡å‹è®­ç»ƒå™¨ï¼ˆé›†æˆæ‰€æœ‰è®­ç»ƒç›¸å…³åŠŸèƒ½ï¼‰"""

    def __init__(self, net=None, device=None, save_every_epoch=False):
        """åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            net: ç¥ç»ç½‘ç»œæ¨¡å‹
            device: è®­ç»ƒè®¾å¤‡
            save_every_epoch: æ˜¯å¦æ¯è½®éƒ½ä¿å­˜æ¨¡å‹
        """
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        VisualizationTool.setup_font()
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = device if device else d2l.try_gpu()  # è‡ªåŠ¨æ£€æµ‹GPU
        self.net = net
        if net:
            self.net.to(self.device)
        self.run_dir = None  # è®­ç»ƒç›®å½•ï¼ˆè®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–ï¼‰
        self.best_test_acc = 0.0  # æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡
        self.total_samples = 0  # ç´¯è®¡å¤„ç†æ ·æœ¬æ•°
        self.save_every_epoch = save_every_epoch  # æ˜¯å¦æ¯è½®éƒ½ä¿å­˜æ¨¡å‹
        if net:
            self._init_weights()  # åˆå§‹åŒ–æƒé‡

    def get_model_config(self, model_type: str, **kwargs) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹çš„é…ç½®å‚æ•°
        
        Args:
            model_type: æ¨¡å‹ç±»å‹åç§°
            **kwargs: å¯é€‰çš„è‡ªå®šä¹‰é…ç½®å‚æ•°
        
        Returns:
            æ¨¡å‹é…ç½®å­—å…¸
        """
        try:
            # ç›´æ¥ä»æ¨¡å‹æ³¨å†Œä¸­å¿ƒè·å–é…ç½®
            default_config = ModelRegistry.get_config(model_type)
            
            # åˆ›å»ºé…ç½®å­—å…¸ï¼Œç¡®ä¿ä¸ä¼šæœ‰Noneå€¼ä¼ é€’ç»™å…³é”®å‚æ•°
            config = {
                "num_epochs": kwargs.get("num_epochs") if kwargs.get("num_epochs") is not None else default_config["num_epochs"],
                "lr": kwargs.get("lr") if kwargs.get("lr") is not None else default_config["lr"],
                "batch_size": kwargs.get("batch_size") if kwargs.get("batch_size") is not None else default_config["batch_size"],
                "resize": kwargs.get("resize") if kwargs.get("resize") is not None else default_config["resize"],
                "input_size": kwargs.get("input_size") if kwargs.get("input_size") is not None else default_config["input_size"],
                "root_dir": kwargs.get("root_dir")
            }
            
            # ç¡®ä¿num_epochsä¸æ˜¯None
            if config["num_epochs"] is None:
                config["num_epochs"] = 10  # é»˜è®¤å€¼
                logger.warning(f"âš ï¸ è­¦å‘Š: {model_type}çš„num_epochsä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼10")
            
            return config
        except ValueError:
            # å¤„ç†é…ç½®ä¸å­˜åœ¨çš„æƒ…å†µ
            logger.warning(f"âš ï¸ æ¨¡å‹ '{model_type}' æ²¡æœ‰é»˜è®¤é…ç½®ï¼Œä½¿ç”¨åŸºç¡€é…ç½®")
            return {
                "num_epochs": kwargs.get("num_epochs", 10),
                "lr": kwargs.get("lr", 0.01),
                "batch_size": kwargs.get("batch_size", 128),
                "input_size": kwargs.get("input_size", (1, 1, 28, 28)),
                "resize": kwargs.get("resize"),
                "root_dir": kwargs.get("root_dir")
            }

    def run_training(self, model_type: str, config: Dict[str, Any], 
                    enable_visualization: bool = True, 
                    save_every_epoch: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨¡å‹è®­ç»ƒå¹¶è¿”å›ç»“æœ
        
        Args:
            model_type: æ¨¡å‹ç±»å‹åç§°
            config: è®­ç»ƒé…ç½®å‚æ•°ï¼ˆåŒ…å«num_epochs, lr, batch_size, resizeç­‰ï¼‰
            enable_visualization: æ˜¯å¦å¯ç”¨å¯è§†åŒ–
            save_every_epoch: æ˜¯å¦æ¯è½®éƒ½ä¿å­˜æ¨¡å‹
        
        Returns:
            Dict: åŒ…å«è®­ç»ƒç»“æœçš„å­—å…¸
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_type}")
        logger.info(f"{'='*60}")
        
        start_time = time.time()
        # è®°å½•å…·ä½“çš„å¼€å§‹è®­ç»ƒæ—¶é—´ç‚¹ï¼ˆæ—¥æœŸ+æ—¶é—´ï¼‰
        training_start_time = datetime.datetime.now()
        result = {
            "model_name": model_type,
            "best_accuracy": 0.0,
            "training_time": 0,
            "error": None,
            "config": config,
            "success": False,
            "run_dir": None,
            "training_start_time": training_start_time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        try:
            # 1. åˆ›å»ºæ¨¡å‹
            net = ModelRegistry.create_model(model_type)
            logger.info(f"ğŸ”§ æ¨¡å‹ {model_type} åˆ›å»ºæˆåŠŸ")
            
            # 2. æµ‹è¯•ç½‘ç»œç»“æ„
            test_func = ModelRegistry.get_test_func(model_type)
            test_func(net, input_size=config["input_size"])
            logger.info(f"âœ… æ¨¡å‹ {model_type} ç½‘ç»œç»“æ„æµ‹è¯•é€šè¿‡")
            
            # 3. åŠ è½½æ•°æ®
            logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®ï¼ˆbatch_size={config['batch_size']}, resize={config['resize']}ï¼‰")
            train_iter, test_iter = DataLoader.load_data(
                batch_size=config["batch_size"], 
                resize=config["resize"]
            )
            
            # 4. åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
            self.net = net
            self.device = d2l.try_gpu()
            self.net.to(self.device)
            self.save_every_epoch = save_every_epoch
            self._init_weights()  # åˆå§‹åŒ–æƒé‡
            
            run_dir, best_acc = self.train(
                train_iter=train_iter,
                test_iter=test_iter,
                num_epochs=config["num_epochs"],
                lr=config["lr"],
                batch_size=config["batch_size"],
                enable_visualization=enable_visualization,
                root_dir=config.get("root_dir")
            )
            
            # 5. è®°å½•ç»“æœ
            training_time = time.time() - start_time
            # è®°å½•å…·ä½“çš„ç»“æŸè®­ç»ƒæ—¶é—´ç‚¹ï¼ˆæ—¥æœŸ+æ—¶é—´ï¼‰
            training_end_time = datetime.datetime.now()
            result.update({
                "best_accuracy": best_acc,
                "training_time": training_time,
                "run_dir": run_dir,
                "success": True,
                "training_end_time": training_end_time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            logger.info(f"ğŸ‰ {model_type} è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}ï¼Œè€—æ—¶: {training_time:.2f}ç§’")
            logger.info(f"â±ï¸ è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"â±ï¸ è®­ç»ƒç»“æŸæ—¶é—´: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        except Exception as e:
            # å¼‚å¸¸å¤„ç†
            training_time = time.time() - start_time
            # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè®°å½•ç»“æŸæ—¶é—´
            training_end_time = datetime.datetime.now()
            error_msg = f"{type(e).__name__}: {str(e)}"
            result.update({
                "training_time": training_time,
                "error": error_msg,
                "success": False,
                "training_end_time": training_end_time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            logger.error(f"âŒ {model_type} è®­ç»ƒå¤±è´¥ï¼é”™è¯¯: {error_msg}")
            logger.info(f"â±ï¸ è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"â±ï¸ è®­ç»ƒç»“æŸæ—¶é—´: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        return result

    def train(
        self,
        train_iter,
        test_iter,
        num_epochs,
        lr,
        batch_size,
        enable_visualization=True,
        **kwargs,
    ):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        Args:
            train_iter: è®­ç»ƒæ•°æ®è¿­ä»£å™¨
            test_iter: æµ‹è¯•æ•°æ®è¿­ä»£å™¨
            num_epochs: è®­ç»ƒè½®æ¬¡
            lr: å­¦ä¹ ç‡
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆç”¨äºä¿å­˜é…ç½®ï¼‰
            enable_visualization: æ˜¯å¦å¯ç”¨å®æ—¶å¯è§†åŒ–ï¼Œé»˜è®¤ä¸ºTrue
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚root_dir
        Returns:
            run_dir: è®­ç»ƒç›®å½•è·¯å¾„
            best_test_acc: æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡
        """
        # 1. åˆå§‹åŒ–è®­ç»ƒç›®å½•å’Œé…ç½®
        # ä»kwargsä¸­è·å–root_dirå‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨Noneï¼ˆé»˜è®¤ä¸ºå½“å‰å·¥ä½œç›®å½•ï¼‰
        root_dir = kwargs.get('root_dir', None)
        self.run_dir = FileUtils.create_run_dir(root_dir=root_dir)
        train_config = {
            "model_name": self.net.__class__.__name__,
            "device": str(self.device),
            "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "num_epochs": num_epochs,
            "learning_rate": lr,
            "batch_size": batch_size,
            "train_samples": len(train_iter) * batch_size,
            "test_samples": len(test_iter) * batch_size,
        }
        FileUtils.save_config(train_config, os.path.join(self.run_dir, "config.json"))

        # 2. åˆå§‹åŒ–è®­ç»ƒç»„ä»¶

        # åˆå§‹åŒ–epochæŒ‡æ ‡åˆ—è¡¨ï¼ˆç”¨äºä¿å­˜æ¯è½®è®­ç»ƒçš„è¯¦ç»†æŒ‡æ ‡ï¼‰
        epoch_metrics = []
        optimizer = torch.optim.SGD(self.net.parameters(), lr=lr)
        loss_fn = nn.CrossEntropyLoss()
        animator = None
        if enable_visualization:
            animator = VisualizationTool.create_animator(
                xlabel="è¿­ä»£å‘¨æœŸ", xlim=[1, num_epochs]
            )
        timer = d2l.Timer()
        num_batches = len(train_iter)

        # 3. å¼€å§‹è®­ç»ƒ
        logger.info(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆè®¾å¤‡: {self.device}ï¼Œè½®æ¬¡: {num_epochs}ï¼‰")
        # è®°å½•è®­ç»ƒå¼€å§‹çš„å…·ä½“æ—¶é—´ç‚¹
        training_start_time = datetime.datetime.now()
        for epoch in range(num_epochs):
            self.net.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
            metric = d2l.Accumulator(3)  # ç´¯è®¡ï¼šæŸå¤±ã€æ­£ç¡®æ•°ã€æ€»æ•°

            for i, (X, y) in enumerate(train_iter):
                timer.start()
                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­+åå‘ä¼ æ’­
                X, y = X.to(self.device), y.to(self.device)
                self.total_samples += X.shape[0]
                y_hat = self.net(X)
                loss = loss_fn(y_hat, y)
                loss.backward()
                optimizer.step()

                # ç´¯è®¡æŒ‡æ ‡
                with torch.no_grad():
                    metric.add(loss * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])

                timer.stop()
                train_loss = metric[0] / metric[2]
                train_acc = metric[1] / metric[2]

                # æ›´æ–°å¯è§†åŒ–ï¼ˆæ¯5ä¸ªæ‰¹æ¬¡æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼‰
                if enable_visualization and (
                    (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1
                ):
                    VisualizationTool.update_realtime(
                        animator,
                        epoch + (i + 1) / num_batches,
                        (train_loss, train_acc, None),
                    )

            # 4. æ¯è½®æ¬¡è¯„ä¼°+ä¿å­˜æ¨¡å‹
            test_acc = self.evaluate_accuracy(test_iter)
            if enable_visualization:
                VisualizationTool.update_realtime(
                    animator, epoch + 1, (None, None, test_acc)
                )
            self._save_best_model(epoch, test_acc)

            # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä¿å­˜æ¯è½®æ¨¡å‹
            if self.save_every_epoch:
                self._save_epoch_model(epoch, test_acc)

            # ä¿å­˜å½“å‰è½®æ¬¡çš„æŒ‡æ ‡
            epoch_metric = {
                "epoch": epoch + 1,  # 1-based epoch
                "train_loss": train_loss,
                "train_acc": train_acc,
                "test_acc": test_acc,
                "best_test_acc": self.best_test_acc,
                "epoch_time": timer.sum(),  # ç´¯è®¡åˆ°å½“å‰è½®çš„æ—¶é—´
            }
            epoch_metrics.append(epoch_metric)

            # æ¯è½®æ¬¡ä¿å­˜epochæŒ‡æ ‡åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆé˜²æ­¢è®­ç»ƒä¸­æ–­æ•°æ®ä¸¢å¤±ï¼‰
            epoch_metrics_path = os.path.join(self.run_dir, "epoch_metrics.json")
            FileUtils.save_metrics(epoch_metrics, epoch_metrics_path)

        # 5. è®­ç»ƒç»“æŸï¼šä¿å­˜æœ€ç»ˆæŒ‡æ ‡ï¼ˆåŒ…å«å®Œæ•´çš„epochæŒ‡æ ‡å†å²ï¼‰
        total_time = timer.sum()
        # è®°å½•è®­ç»ƒç»“æŸçš„å…·ä½“æ—¶é—´ç‚¹
        training_end_time = datetime.datetime.now()
        final_metrics = {
            "final_train_loss": train_loss,
            "final_train_acc": train_acc,
            "final_test_acc": test_acc,
            "best_test_acc": self.best_test_acc,
            "total_training_time": f"{total_time:.2f}s",
            "samples_per_second": f"{self.total_samples / total_time:.1f}",
            "epoch_metrics": epoch_metrics,  # åŒ…å«å®Œæ•´çš„æ¯è½®æŒ‡æ ‡å†å²
            "training_start_time": training_start_time.strftime("%Y-%m-%d %H:%M:%S"),
            "training_end_time": training_end_time.strftime("%Y-%m-%d %H:%M:%S")
        }
        FileUtils.save_metrics(final_metrics, os.path.join(self.run_dir, "metrics.json"))

        # 6. è¾“å‡ºè®­ç»ƒæ€»ç»“
        logger.info("\n" + "=" * 80)
        logger.info(f"ğŸ“ è®­ç»ƒæ€»ç»“ï¼ˆç›®å½•: {self.run_dir}ï¼‰")
        logger.info(
            f"loss: {train_loss:.4f} | è®­ç»ƒacc: {train_acc:.4f} | æµ‹è¯•acc: {test_acc:.4f}")
        logger.info(
            f"æœ€ä½³acc: {self.best_test_acc:.4f} | æ€»æ—¶é—´: {total_time:.2f}s | é€Ÿåº¦: {self.total_samples/total_time:.1f}æ ·æœ¬/ç§’")
        logger.info("=" * 80)

        return self.run_dir, self.best_test_acc

    def run_post_training_prediction(self, run_dir: str, n: int = 8, num_samples: int = 10) -> None:
        """
        è®­ç»ƒåè‡ªåŠ¨è¿›è¡Œé¢„æµ‹å¯è§†åŒ–
        
        Args:
            run_dir: è®­ç»ƒç›®å½•è·¯å¾„
            n: å¯è§†åŒ–æ ·æœ¬æ•°
            num_samples: éšæœºæµ‹è¯•æ ·æœ¬æ•°
        """
        logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼Œå¼€å§‹é¢„æµ‹å¯è§†åŒ–ï¼ˆç›®å½•: {run_dir}ï¼‰")
        predictor = Predictor.from_run_dir(run_dir)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šresizeå‚æ•°
        resize = None  # é»˜è®¤LeNetä¸éœ€è¦resize
        if predictor.config and "model_name" in predictor.config:
            if predictor.config["model_name"] == "AlexNet":
                resize = 224  # AlexNetéœ€è¦224x224è¾“å…¥
            elif predictor.config["model_name"] == "VGG":
                resize = 224  # VGGéœ€è¦224x224è¾“å…¥
            elif predictor.config["model_name"] == "GoogLeNet":
                resize = 96   # GoogLeNetéœ€è¦96x96è¾“å…¥
        
        # é‡æ–°åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨æ­£ç¡®çš„resizeå‚æ•°ï¼‰
        _, test_iter_pred = DataLoader.load_data(batch_size=256, resize=resize)
        
        # æ‰§è¡Œé¢„æµ‹å¯è§†åŒ–
        predictor.visualize_prediction(test_iter_pred, n=n)
        predictor.test_random_input(num_samples=num_samples)

    def _init_weights(self):
        """Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡ï¼ˆé€‚é…Linear/Conv2dï¼‰"""

        def init_func(m):
            if isinstance(m, (nn.Linear, nn.Conv2d)):
                nn.init.xavier_uniform_(m.weight)

        self.net.apply(init_func)

    def evaluate_accuracy(self, data_iter):
        """è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡"""
        self.net.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        metric = d2l.Accumulator(2)  # ç´¯è®¡ï¼šæ­£ç¡®æ•°ã€æ€»æ•°

        with torch.no_grad():
            for X, y in data_iter:
                X, y = X.to(self.device), y.to(self.device)
                metric.add(d2l.accuracy(self.net(X), y), y.numel())

        return metric[0] / metric[1]

    def _save_best_model(self, epoch, current_test_acc):
        """ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ–‡ä»¶åå«å‡†ç¡®ç‡å’Œepochï¼‰"""
        if current_test_acc <= self.best_test_acc:
            return None  # æœªè¶…è¿‡æœ€ä½³ï¼Œä¸ä¿å­˜

        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        self.best_test_acc = current_test_acc

        # åˆ é™¤ä¹‹å‰çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼ˆç¡®ä¿æœ€ç»ˆåªæœ‰ä¸€ä¸ªæœ€ä½³æ¨¡å‹ï¼‰
        best_model_pattern = f"best_model_{self.net.__class__.__name__}_*.pth"
        for filename in os.listdir(self.run_dir):
            if re.match(best_model_pattern.replace("*", ".*"), filename):
                old_model_path = os.path.join(self.run_dir, filename)
                try:
                    os.remove(old_model_path)
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹: {filename}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤æ—§æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")

        # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
        model_filename = (
            f"best_model_{self.net.__class__.__name__}_"
            f"acc_{self.best_test_acc:.4f}_epoch_{epoch+1}.pth"
        )
        model_path = os.path.join(self.run_dir, model_filename)

        # ä¿å­˜å®Œæ•´çŠ¶æ€ï¼ˆå«æƒé‡ã€å‡†ç¡®ç‡ã€epochï¼‰
        torch.save(
            {
                "model_state_dict": self.net.state_dict(),
                "best_test_acc": self.best_test_acc,
                "epoch": epoch + 1,  # 1-based epoch
                "device": str(self.device),
            },
            model_path,
        )

        logger.info(f"ğŸ“Œ ä¿å­˜æœ€ä½³æ¨¡å‹: {model_filename}ï¼ˆå‡†ç¡®ç‡: {self.best_test_acc:.4f}ï¼‰")
        return model_path

    def _save_epoch_model(self, epoch, current_test_acc):
        """ä¿å­˜æ¯è½®æ¬¡æ¨¡å‹ï¼ˆæ— è®ºæ˜¯å¦æœ€ä½³ï¼‰"""
        model_filename = (
            f"epoch_model_{self.net.__class__.__name__}_"
            f"acc_{current_test_acc:.4f}_epoch_{epoch+1}.pth"
        )
        model_path = os.path.join(self.run_dir, model_filename)

        # ä¿å­˜å®Œæ•´çŠ¶æ€
        torch.save(
            {
                "model_state_dict": self.net.state_dict(),
                "test_acc": current_test_acc,
                "epoch": epoch + 1,  # 1-based epoch
                "device": str(self.device),
            },
            model_path,
        )

        logger.info(f"ğŸ’¾ ä¿å­˜è½®æ¬¡æ¨¡å‹: {model_filename}ï¼ˆå‡†ç¡®ç‡: {current_test_acc:.4f}ï¼‰")
        return model_path