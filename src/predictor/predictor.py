import torch
from d2l import torch as d2l
import os
import sys
import json
import matplotlib.pyplot as plt
import glob
from typing import Optional, Dict, Any, List
from src.models.lenet import LeNet, LeNetBatchNorm
from src.models.alexnet import AlexNet
from src.models.vgg import VGG
from src.models.nin import NIN
from src.models.googlenet import GoogLeNet
from src.models.resnet import ResNet
from src.models.dense_net import DenseNet
from src.models.mlp import MLP
from src.utils.file_utils import FileUtils
from src.utils.visualization import VisualizationTool
from src.utils.data_utils import DataLoader

from src.utils.log_utils.log_utils import get_logger


# åˆå§‹åŒ–æ—¥å¿—ï¼Œè®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
logger = get_logger(
    name=__name__,
    log_file=f"logs/predictor.log",  # æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºlogsç›®å½•
    global_level="DEBUG",     # å…¨å±€æ—¥å¿—çº§åˆ«
)

# ========================= å¢å¼ºç‰ˆæ¨¡å‹é¢„æµ‹ç±» =========================
class Predictor:
    """å¢å¼ºç‰ˆæ¨¡å‹é¢„æµ‹ç±»ï¼šæ•´åˆäº†é¢„æµ‹æ ¸å¿ƒåŠŸèƒ½å’Œé«˜çº§å·¥å…·åŠŸèƒ½"""

    def __init__(self, net, device=None):
        self.net = net
        self.device = device if device else d2l.try_gpu()
        self.net.to(self.device)
        self.net.eval()  # åˆå§‹åŒ–å³åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        self.config = None  # åŠ è½½çš„è®­ç»ƒé…ç½®
        self.best_acc = None  # æ¨¡å‹æœ€ä½³å‡†ç¡®ç‡
        self.run_dir = None  # è®­ç»ƒç›®å½•è·¯å¾„
        self.model_path = None  # æ¨¡å‹æ–‡ä»¶è·¯å¾„
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        VisualizationTool.setup_font()

    @classmethod
    def from_run_dir(cls, run_dir, device=None, model_file=None):
        """
        ä»è®­ç»ƒç›®å½•åˆ›å»ºPredictorï¼ˆå¯é€‰æ‹©åŠ è½½ç‰¹å®šæ¨¡å‹æ–‡ä»¶ï¼‰
        Args:
            run_dir: è®­ç»ƒç›®å½•è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹GPUï¼‰
            model_file: å¯é€‰ï¼ŒæŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶å
        Returns:
            Predictorå®ä¾‹
        """
        # 1. éªŒè¯ç›®å½•
        FileUtils.validate_directory(run_dir)

        config_path = os.path.join(run_dir, "config.json")
        FileUtils.validate_file(config_path, ".json")

        # 2. ç¡®å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„
        if model_file:
            # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶
            model_path = os.path.join(run_dir, model_file)
            FileUtils.validate_file(model_path, ".pth")
        else:
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹æ–‡ä»¶
            model_file = FileUtils.find_best_model_in_dir(run_dir)
            model_path = os.path.join(run_dir, model_file)
            
        # 3. å¤ç”¨from_model_pathæ–¹æ³•åŠ è½½æ¨¡å‹å’Œé…ç½®
        # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥äº†æ˜ç¡®çš„config_pathï¼Œç¡®ä¿é…ç½®èƒ½è¢«æ­£ç¡®åŠ è½½
        predictor = cls.from_model_path(model_path, config_path=config_path, device=device)
        predictor.run_dir = run_dir
        return predictor

    @classmethod
    def from_model_path(cls, model_path, config_path=None, device=None):
        """
        ä»æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„åˆ›å»ºPredictor
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰
            config_path: å¯é€‰ï¼Œé…ç½®æ–‡ä»¶è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹GPUï¼‰
        Returns:
            Predictorå®ä¾‹
        """
        # 1. éªŒè¯å¹¶è§„èŒƒåŒ–æ¨¡å‹æ–‡ä»¶è·¯å¾„
        if not model_path:
            raise ValueError("æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
            
        # ç¡®ä¿è·¯å¾„è§„èŒƒåŒ–ï¼ˆå¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
        model_path = FileUtils.normalize_path(model_path)
        FileUtils.validate_file(model_path, ".pth")

        # 2. åŠ è½½æ¨¡å‹ï¼ˆæ·»åŠ ç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†ï¼‰
        try:
            # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒweights_onlyå‚æ•°
            try:
                # å°è¯•ä½¿ç”¨weights_onlyå‚æ•°
                checkpoint = torch.load(model_path, map_location="cpu", weights_only=True)
            except TypeError:
                # å¦‚æœæŠ¥é”™ï¼ˆä¸æ”¯æŒè¯¥å‚æ•°ï¼‰ï¼Œåˆ™ä¸ä½¿ç”¨weights_only
                checkpoint = torch.load(model_path, map_location="cpu")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹æ–‡ä»¶å¤±è´¥: {str(e)}") from e

        # éªŒè¯checkpointå†…å®¹
        if "model_state_dict" not in checkpoint:
            raise ValueError(f"æ¨¡å‹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘'model_state_dict'é”®: {model_path}")

        # 3. å°è¯•è‡ªåŠ¨ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
        if not config_path:
            # å‡è®¾é…ç½®æ–‡ä»¶åœ¨åŒä¸€ç›®å½•
            config_path = FileUtils.get_config_path_from_model_path(model_path)

        # 4. åŠ è½½é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
        config = {}
        if os.path.exists(config_path):
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config = json.load(f)
                logger.info(f"âœ… åŠ è½½é…ç½®: {os.path.basename(config_path)}")
                model_name = config.get("model_name", "LeNet")  # é»˜è®¤LeNet
            except Exception as e:
                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®: {str(e)}")
                model_name = "LeNet"
                config["model_name"] = model_name
        else:
            logger.info("âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ç±»å‹")
            model_name = "LeNet"
            # å³ä½¿æ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œä¹Ÿè¦è®¾ç½®model_name
            config["model_name"] = model_name

        # 5. åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        try:
            if model_name == "LeNet":
                net = LeNet()
            elif model_name == "LeNetBatchNorm":
                net = LeNetBatchNorm()
            elif model_name == "AlexNet":
                net = AlexNet()
            elif model_name == "VGG":
                net = VGG()
            elif model_name == "NIN":
                net = NIN()
            elif model_name == "GoogLeNet":
                net = GoogLeNet()
            elif model_name == "ResNet":
                net = ResNet()
            elif model_name == "DenseNet":
                net = DenseNet()
            elif model_name == "MLP":
                net = MLP()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}")

            net.load_state_dict(checkpoint["model_state_dict"])
            best_acc = checkpoint.get("best_test_acc", 0.0)
            logger.info(f"âœ… åŠ è½½æ¨¡å‹: {os.path.basename(model_path)}ï¼ˆå‡†ç¡®ç‡: {best_acc:.4f}ï¼‰")
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºæ¨¡å‹æˆ–åŠ è½½æƒé‡å¤±è´¥: {str(e)}") from e

        # 6. è¿”å›Predictorå®ä¾‹
        predictor = cls(net, device)
        predictor.config = config
        predictor.best_acc = best_acc
        predictor.model_path = model_path
        return predictor
    
    @classmethod
    def create_from_args(cls, run_dir=None, model_file=None, model_path=None, root_dir=None, device=None):
        """
        ä»å‚æ•°åˆ›å»ºPredictorå®ä¾‹ï¼Œæ”¯æŒå¤šç§åˆ›å»ºæ–¹å¼
        Args:
            run_dir: è®­ç»ƒç›®å½•è·¯å¾„
            model_file: æ¨¡å‹æ–‡ä»¶å
            model_path: å®Œæ•´æ¨¡å‹æ–‡ä»¶è·¯å¾„
            root_dir: æ ¹ç›®å½•è·¯å¾„ï¼Œç”¨äºè‡ªåŠ¨æŸ¥æ‰¾è®­ç»ƒç›®å½•
            device: è¿è¡Œè®¾å¤‡
        Returns:
            Predictorå®ä¾‹
        """
        if model_path:
            # ç›´æ¥ä»æ¨¡å‹æ–‡ä»¶è·¯å¾„åŠ è½½
            logger.info(f"ğŸ” æ¨¡å¼ï¼šä»æ¨¡å‹æ–‡ä»¶ç›´æ¥åŠ è½½")
            return cls.from_model_path(model_path, device=device)
        else:
            # å¦‚æœæœªæŒ‡å®šè®­ç»ƒç›®å½•ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€æ–°ç›®å½•
            if not run_dir:
                # æœç´¢åŒ…å«/runsç›®å½•çš„å¤šä¸ªä½ç½®
                search_dirs = [
                    root_dir,  # å½“å‰ç›®å½•
                    os.path.join(root_dir, "data"),  # dataç›®å½•
                    os.path.join(root_dir, "runs")  # runsç›®å½•
                ]
                logger.info(f"ğŸ” æœç´¢ç›®å½•: {search_dirs}")
                run_dir = FileUtils.find_latest_run_dir(root_dir=root_dir, search_dirs=search_dirs)
            
            # ä»è®­ç»ƒç›®å½•åŠ è½½ï¼ˆå¯é€‰æ‹©æ¨¡å‹æ–‡ä»¶ï¼‰
            logger.info(f"ğŸ” æ¨¡å¼ï¼šä»è®­ç»ƒç›®å½•åŠ è½½{', è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹' if not model_file else f', æŒ‡å®šæ¨¡å‹æ–‡ä»¶: {model_file}'}")
            return cls.from_run_dir(run_dir, model_file=model_file, device=device)



    def get_resize_for_model(self) -> Optional[int]:
        """
        æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šåˆé€‚çš„resizeå‚æ•°
        Returns:
            é€‚åˆè¯¥æ¨¡å‹çš„resizeå€¼æˆ–None
        """
        resize = None  # é»˜è®¤ä¸éœ€è¦resize
        if self.config and "model_name" in self.config:
            if self.config["model_name"] == "AlexNet" or self.config["model_name"] == "VGG":
                resize = 224  # AlexNetã€VGGéœ€è¦224x224è¾“å…¥
            elif self.config["model_name"] == "GoogLeNet" or "DenseNet" in self.config["model_name"]:
                resize = 96   # GoogLeNetå’ŒDenseNetéœ€è¦96x96è¾“å…¥
            else:
                resize = 224  # å…¶ä»–æ¨¡å‹é»˜è®¤224x224
        return resize
        
    def list_models_in_directory(self) -> None:
        """
        åˆ—å‡ºè®­ç»ƒç›®å½•ä¸­çš„æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
        """
        if not self.run_dir:
            logger.info("âš ï¸ æœªè®¾ç½®è®­ç»ƒç›®å½•ï¼Œæ— æ³•åˆ—å‡ºæ¨¡å‹")
            return
        
        try:
            models_info = FileUtils.list_models_in_dir(self.run_dir)
            logger.info(f"\nğŸ“‹ {self.run_dir} ç›®å½•ä¸­çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰å‡†ç¡®ç‡æ’åºï¼‰:")
            logger.info(f"{'åºå·':<4} {'æ–‡ä»¶å':<60} {'å‡†ç¡®ç‡':<10} {'è½®æ¬¡':<6}")
            logger.info("-" * 80)
            for i, model_info in enumerate(models_info, 1):
                # æ ‡è®°å½“å‰åŠ è½½çš„æœ€ä½³æ¨¡å‹
                mark = "â­" if i == 1 else " "
                logger.info(
                    f"{i:<4} {model_info['filename']:<60} {model_info['accuracy']:.4f}    {model_info['epoch']:<6} {mark}")

            # æç¤ºç”¨æˆ·å¯ä»¥é€šè¿‡model_fileå‚æ•°æŒ‡å®šå…·ä½“æ¨¡å‹
            if len(models_info) > 1:
                logger.info(f"\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ model_file å‚æ•°å¯ä»¥åŠ è½½ç‰¹å®šæ¨¡å‹ï¼Œä¾‹å¦‚:")
                logger.info(
                    f"   predict --run_dir='{self.run_dir}' --model_file='{models_info[1]['filename']}'")
        except Exception as e:
            logger.exception(f"âš ï¸ åˆ—å‡ºæ¨¡å‹æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

    def predict(self, X):
        """åŸºç¡€é¢„æµ‹ï¼šè¿”å›é¢„æµ‹ç±»åˆ«ï¼ˆ1Då¼ é‡ï¼‰"""
        with torch.no_grad():
            X = X.to(self.device)
            return torch.argmax(self.net(X), dim=1)

    def visualize_prediction(self, test_iter, n=8):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœï¼ˆæ­£ç¡®ç»¿è‰²/é”™è¯¯çº¢è‰²æ ‡è®°ï¼‰"""
        logger.info(f"\nğŸ“Š å¯è§†åŒ– {n} ä¸ªæµ‹è¯•æ ·æœ¬é¢„æµ‹ç»“æœï¼ˆè®¾å¤‡: {self.device}ï¼‰")

        # è·å–æµ‹è¯•æ ·æœ¬
        X, y = next(iter(test_iter))
        X, y = X[:n], y[:n]
        y_hat = self.predict(X)

        # è½¬æ¢æ ‡ç­¾ä¸ºæ–‡æœ¬
        true_labels = d2l.get_fashion_mnist_labels(y)
        pred_labels = d2l.get_fashion_mnist_labels(y_hat.cpu())

        # ç”Ÿæˆç®€æ´çš„æ ‡é¢˜ï¼ˆé¿å…é‡å ï¼‰
        titles = []
        for t, p, y_true, y_pred in zip(true_labels, pred_labels, y, y_hat.cpu()):
            status = "å¯¹" if y_true == y_pred else "é”™"
            titles.append(f"{status}\nçœŸå®:{t}\né¢„æµ‹:{p}")

        # ç¡®å®šå›¾åƒå°ºå¯¸ - æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è°ƒæ•´
        image_size = 28  # é»˜è®¤LeNetçš„28Ã—28
        if self.config and "model_name" in self.config:
            if self.config["model_name"] == "AlexNet" or self.config["model_name"] == "VGG":
                image_size = 224  # AlexNetã€VGGä½¿ç”¨224Ã—224è¾“å…¥
            elif self.config["model_name"] == "GoogLeNet" or "DenseNet" in self.config["model_name"]:
                image_size = 96   # GoogLeNetå’ŒDenseNetä½¿ç”¨96Ã—96è¾“å…¥

        # é‡å¡‘å›¾åƒå¹¶æ˜¾ç¤º
        X_reshaped = X.reshape((n, image_size, image_size))

        # ä½¿ç”¨å¢å¼ºç‰ˆçš„show_imagesæ–¹æ³•æ˜¾ç¤ºå›¾åƒ
        VisualizationTool.show_images(
            X_reshaped,
            num_rows=1,
            num_cols=n,
            titles=titles,
            scale=1.8,  # å¢åŠ ç¼©æ”¾å› å­ä»¥æä¾›æ›´å¤šç©ºé—´æ˜¾ç¤ºæ ‡ç­¾
            figsize=(n * 1.5, 3),  # å¢åŠ å›¾è¡¨é«˜åº¦ï¼Œç¡®ä¿æ ‡é¢˜å®Œå…¨æ˜¾ç¤º
        )
        plt.show()

        # è¾“å‡ºé¢„æµ‹è¯¦æƒ…
        correct = torch.sum(y == y_hat.cpu()).item()
        logger.info(f"\nğŸ“‹ é¢„æµ‹è¯¦æƒ…:")
        logger.info(f"çœŸå®æ ‡ç­¾: {y.tolist()} â†’ {true_labels}")
        logger.info(f"é¢„æµ‹æ ‡ç­¾: {y_hat.tolist()} â†’ {pred_labels}")
        logger.info(f"é¢„æµ‹æ­£ç¡®ç‡: {correct / n:.2%}ï¼ˆ{correct}/{n}ï¼‰")

    def test_random_input(self, num_samples=10):
        """æµ‹è¯•éšæœºè¾“å…¥ï¼ˆéªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼‰"""
        # ç¡®å®šè¾“å…¥å°ºå¯¸ - æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è°ƒæ•´
        input_size = (1, 28, 28)  # é»˜è®¤LeNetçš„è¾“å…¥å°ºå¯¸
        if self.config and "model_name" in self.config:
            if self.config["model_name"] == "AlexNet" or self.config["model_name"] == "VGG":
                input_size = (1, 224, 224)
            elif self.config["model_name"] == "GoogLeNet" or "DenseNet" in self.config["model_name"]:
                input_size = (1, 96, 96)

        logger.info(
            f"\nğŸ” æµ‹è¯• {num_samples} ä¸ªéšæœºè¾“å…¥ï¼ˆ{input_size[1]}x{input_size[2]}ç°åº¦å›¾ï¼‰"
        )
        random_X = torch.randn(num_samples, *input_size)  # æ¨¡æ‹Ÿéšæœºå›¾åƒ
        random_preds = self.predict(random_X)
        random_labels = d2l.get_fashion_mnist_labels(random_preds.cpu())

        # è·å–æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        with torch.no_grad():
            random_X = random_X.to(self.device)
            outputs = self.net(random_X)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            max_probs, _ = torch.max(probabilities, dim=1)
            max_probs = max_probs.cpu().tolist()
        
        # æŒ‰ç…§visualize_predictionçš„æ ¼å¼è¾“å‡ºé¢„æµ‹è¯¦æƒ…
        logger.info(f"\nğŸ“‹ éšæœºè¾“å…¥é¢„æµ‹è¯¦æƒ…:")
        logger.info(f"é¢„æµ‹ç±»åˆ«: {random_preds.tolist()} â†’ {random_labels}")
        logger.info(f"é¢„æµ‹ç½®ä¿¡åº¦: {[f'{p:.2f}' for p in max_probs]}")
        
        # è¯†åˆ«é«˜ç½®ä¿¡åº¦é¢„æµ‹ï¼ˆç½®ä¿¡åº¦>0.5ï¼‰
        high_confidence_count = sum(1 for p in max_probs if p > 0.5)
        logger.info(f"é«˜ç½®ä¿¡åº¦é¢„æµ‹({high_confidence_count}/{num_samples}): ç½®ä¿¡åº¦>0.5")

        # æ£€æŸ¥é¢„æµ‹å¤šæ ·æ€§ï¼ˆé¿å…æ¨¡å‹è¾“å‡ºå•ä¸€ç±»åˆ«ï¼‰
        unique_preds = torch.unique(random_preds).numel()
        if unique_preds < 3:
            logger.info(f"âš ï¸ è­¦å‘Š: éšæœºé¢„æµ‹ç±»åˆ«è¾ƒå°‘ï¼ˆ{unique_preds}ç§ï¼‰ï¼Œæ¨¡å‹å¯èƒ½æœªå……åˆ†è®­ç»ƒ")
        else:
            logger.info(f"âœ… éšæœºé¢„æµ‹ç±»åˆ«å¤šæ ·ï¼ˆ{unique_preds}ç§ï¼‰ï¼Œæ¨¡å‹çŠ¶æ€æ­£å¸¸")
            
    def run_prediction(self, batch_size: int = 256, num_samples: int = 10) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨¡å‹é¢„æµ‹å¹¶è¿”å›ç»“æœ
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            num_samples: éšæœºæµ‹è¯•æ ·æœ¬æ•°
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # å¦‚æœæœ‰è®­ç»ƒç›®å½•ä¸”æœªæŒ‡å®šæ¨¡å‹æ–‡ä»¶ï¼Œæ˜¾ç¤ºè¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
        if self.run_dir and not (self.model_path and os.path.basename(self.model_path) != FileUtils.find_best_model_in_dir(self.run_dir)):
            self.list_models_in_directory()
        
        # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šresizeå‚æ•°
        resize = self.get_resize_for_model()
        
        # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨æ­£ç¡®çš„resizeå‚æ•°ï¼‰
        _, test_iter = DataLoader.load_data(
            batch_size=batch_size, resize=resize
        )
        
        # æ‰§è¡Œé¢„æµ‹å¯è§†åŒ–
        self.visualize_prediction(test_iter, n=num_samples)
        self.test_random_input(num_samples=num_samples)
        
        # è¿”å›é¢„æµ‹ç»“æœä¿¡æ¯
        result = {
            "success": True,
            "model_name": self.config["model_name"] if self.config else "æœªçŸ¥",
            "run_dir": self.run_dir,
            "model_path": self.model_path
        }
        
        return result

        