import os
import json
import glob
import torch
import matplotlib.pyplot as plt
import sys
import numpy as np
from typing import List, Dict, Optional, Any, Callable, Type
from functools import lru_cache
from abc import ABC, abstractmethod
from dataclasses import dataclass

# è§£å†³OpenMPè¿è¡Œæ—¶åº“å†²çªé—®é¢˜
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# å¯¼å…¥è‡ªå®šä¹‰æ—¥å¿—æ¨¡å—
from src.utils.log_utils import get_logger

# åˆå§‹åŒ–æ—¥å¿—å™¨
logger = get_logger(name=__name__, log_file="logs/model_analysis.log", global_level="INFO")


# å¯¼å…¥å·¥å…·æ³¨å†Œä¸­å¿ƒ
from src.helper_utils.helper_tools_registry import ToolRegistry

# å¯¼å…¥æ‹†åˆ†çš„æ¨¡å—
from src.model_helper_utils.metric_parsers import MetricParserRegistry
from src.model_helper_utils.visualizers import VisualizerRegistry


class PathScanner:
    """è·¯å¾„æ‰«ææ¨¡å—ï¼šè´Ÿè´£æŸ¥æ‰¾è®­ç»ƒç›®å½•å’Œå„ç±»æ–‡ä»¶"""

    @staticmethod
    @lru_cache(maxsize=None)  # ç¼“å­˜ç›®å½•æŸ¥æ‰¾ç»“æœï¼Œå‡å°‘é‡å¤IO
    def find_run_directories(
        pattern: str = "run_*", root_dir: str = "."  # æ”¯æŒè‡ªå®šä¹‰æ ¹ç›®å½•
    ) -> List[str]:
        """æ ¹æ®æ¨¡å¼æŸ¥æ‰¾è®­ç»ƒç›®å½•"""
        dir_pattern = os.path.join(root_dir, pattern)
        all_entries = glob.glob(dir_pattern)
        return [entry for entry in all_entries if os.path.isdir(entry)]

    @staticmethod
    def find_model_files(directory: str, pattern: str = "*.pth") -> List[str]:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶ï¼ˆéé€’å½’ï¼‰"""
        if not os.path.exists(directory):
            return []
        return glob.glob(os.path.join(directory, pattern))

    @staticmethod
    def find_metric_files(directory: str, pattern: str = "*.json") -> List[str]:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æŒ‡æ ‡æ–‡ä»¶ï¼ˆéé€’å½’ï¼‰"""
        if not os.path.exists(directory):
            return []
        return glob.glob(os.path.join(directory, pattern))

    @staticmethod
    def get_latest_run_directory(
        pattern: str = "run_*", root_dir: str = "."
    ) -> Optional[str]:
        """è·å–æœ€æ–°ä¿®æ”¹çš„è®­ç»ƒç›®å½•"""
        run_dirs = PathScanner.find_run_directories(pattern, root_dir)
        if not run_dirs:
            return None
        return max(run_dirs, key=lambda x: os.path.getmtime(x))


# ==================================================
# ä¸šåŠ¡é€»è¾‘å±‚ï¼šé…ç½®åŠ è½½ã€æŒ‡æ ‡æå–ä¸æ¨¡å‹ä¿¡æ¯ç®¡ç†
# ==================================================
class ConfigLoader:
    """é…ç½®åŠ è½½æ¨¡å—ï¼šè´Ÿè´£åŠ è½½å’Œè§£æé…ç½®æ–‡ä»¶ä¸æ¨¡å‹æ£€æŸ¥ç‚¹"""

    @staticmethod
    def _load_file_safely(
        loader: Callable[[str], Any], file_path: str, error_prefix: str
    ) -> Optional[Any]:
        """é€šç”¨å®‰å…¨åŠ è½½å‡½æ•°ï¼šç»Ÿä¸€å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨å’ŒåŠ è½½å¼‚å¸¸"""
        if not os.path.exists(file_path):
            return None
        try:
            return loader(file_path)
        except Exception as e:
            logger.warning(f"{error_prefix} {file_path}: {str(e)}")
            return None

    @staticmethod
    def load_run_config(run_dir: str) -> Optional[Dict[str, Any]]:
        """ä»è®­ç»ƒç›®å½•åŠ è½½config.json"""
        config_path = os.path.join(run_dir, "config.json")

        def _json_loader(path: str) -> Dict[str, Any]:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)

        return ConfigLoader._load_file_safely(
            _json_loader, config_path, "åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥"
        )

    @staticmethod
    def load_model_checkpoint(model_path: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆCPUåŠ è½½é¿å…è®¾å¤‡ä¸åŒ¹é…ï¼‰"""

        def _model_loader(path: str) -> Dict[str, Any]:
            # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒweights_onlyå‚æ•°
            try:
                return torch.load(path, map_location="cpu", weights_only=True)
            except TypeError:
                return torch.load(path, map_location="cpu")

        return ConfigLoader._load_file_safely(
            _model_loader, model_path, "åŠ è½½æ¨¡å‹æ–‡ä»¶å¤±è´¥"
        )

    @staticmethod
    def get_model_type(config: Optional[Dict[str, Any]]) -> str:
        """ä»é…ç½®ä¸­æå–æ¨¡å‹ç±»å‹"""
        if not config:
            return "Unknown"
        return config.get("model_name", "Unknown")


class MetricExtractor:
    """æŒ‡æ ‡æå–æ¨¡å—ï¼šè´Ÿè´£ä»æ–‡ä»¶ä¸­æå–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""

    @staticmethod
    def extract_run_metrics(run_dir: str) -> Optional[Dict[str, Any]]:
        """ä»metrics.jsonæå–è®­ç»ƒæŒ‡æ ‡"""
        metrics_path = os.path.join(run_dir, "metrics.json")

        def _metrics_loader(path: str) -> Dict[str, Any]:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)

        metrics = ConfigLoader._load_file_safely(
            _metrics_loader, metrics_path, "æå–è¿è¡ŒæŒ‡æ ‡å¤±è´¥"
        )
        if not metrics:
            return None

        return {
            "best_acc": float(metrics.get("best_test_acc", 0.0)),
            "final_acc": float(metrics.get("final_test_acc", 0.0)),
            "time_cost": metrics.get("total_training_time", "N/A"),
        }

    @staticmethod
    def extract_model_metrics(checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æ¨¡å‹æ£€æŸ¥ç‚¹æå–æŒ‡æ ‡ï¼ˆå…¼å®¹ä¸åŒæ ¼å¼ï¼‰"""
        return {
            "best_acc": float(
                checkpoint.get("best_test_acc", checkpoint.get("test_acc", 0.0))
            ),
            "epoch": int(checkpoint.get("epoch", 0)),
            "device": checkpoint.get("device", "N/A"),
        }


class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯æ¨¡å—ï¼šæ•´åˆè·¯å¾„ã€é…ç½®å’ŒæŒ‡æ ‡ä¿¡æ¯"""

    @staticmethod
    def from_run_directory(run_dir: str) -> Optional[Dict[str, Any]]:
        """ä»è®­ç»ƒç›®å½•ç”Ÿæˆä»»åŠ¡çº§ä¿¡æ¯"""
        config = ConfigLoader.load_run_config(run_dir)
        metrics = MetricExtractor.extract_run_metrics(run_dir)

        if not config or not metrics:
            return None

        return {
            "type": "run",
            "dir": run_dir,
            "model_type": ConfigLoader.get_model_type(config),
            "params": {
                "lr": config.get("learning_rate"),
                "batch_size": config.get("batch_size"),
                "epochs": config.get("num_epochs"),
                "timestamp": config.get("timestamp"),
            },
            "metrics": metrics,
        }

    @staticmethod
    def from_model_file(model_path: str) -> Optional[Dict[str, Any]]:
        """ä»æ¨¡å‹æ–‡ä»¶ç”Ÿæˆå¿«ç…§çº§ä¿¡æ¯"""
        checkpoint = ConfigLoader.load_model_checkpoint(model_path)
        if not checkpoint:
            return None

        # å°è¯•ä»æ¨¡å‹æ‰€åœ¨ç›®å½•åŠ è½½é…ç½®
        model_dir = os.path.dirname(model_path)
        config = ConfigLoader.load_run_config(model_dir)

        return {
            "type": "model",
            "path": model_path,
            "filename": os.path.basename(model_path),
            "model_type": ConfigLoader.get_model_type(config),
            "params": {
                "lr": config.get("learning_rate") if config else None,
                "batch_size": config.get("batch_size") if config else None,
            },
            "metrics": MetricExtractor.extract_model_metrics(checkpoint),
        }


# ==================================================
# æœåŠ¡ç¼–æ’å±‚ï¼šæ•´åˆå„æ¨¡å—æä¾›å®Œæ•´åˆ†ææœåŠ¡
# ==================================================
class ResultVisualizer: 
    """ç»“æœå±•ç¤ºæ¨¡å—ï¼šè´Ÿè´£æ’åºå’Œæ ¼å¼åŒ–è¾“å‡ºç»“æœ"""

    @staticmethod
    def sort_by_metric(
        items: List[Dict[str, Any]], metric_key: str = "best_acc", reverse: bool = True
    ) -> List[Dict[str, Any]]:
        """æŒ‰æŒ‡å®šæŒ‡æ ‡æ’åºï¼ˆé»˜è®¤æŒ‰æœ€ä½³å‡†ç¡®ç‡é™åºï¼‰"""
        valid_items = [item for item in items if metric_key in item["metrics"]]
        return sorted(
            valid_items, key=lambda x: x["metrics"][metric_key], reverse=reverse
        )

    @staticmethod
    def print_summary_table(items: List[Dict[str, Any]], top_n: int = 10) -> None:
        """æ‰“å°æ ¼å¼åŒ–æ±‡æ€»è¡¨æ ¼ï¼ˆé€‚é…ä»»åŠ¡çº§/æ¨¡å‹çº§ä¿¡æ¯ï¼‰"""
        if not items:
            logger.info("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å±•ç¤º")
            return

        is_run_summary = items[0]["type"] == "run"
        display_items = items[:top_n]

        # è¡¨æ ¼æ ‡é¢˜
        logger.info("\n" + "=" * 120)
        logger.info(f"ğŸ“Š åˆ†æç»“æœæ±‡æ€»ï¼ˆå…± {len(items)} é¡¹ï¼Œæ˜¾ç¤ºå‰ {len(display_items)} é¡¹ï¼‰")
        logger.info("=" * 120)

        # è¡¨å¤´ï¼ˆæŒ‰ä¿¡æ¯ç±»å‹åŒºåˆ†ï¼‰
        if is_run_summary:
            headers = [
                "æ’å", "ç›®å½•å", "æ¨¡å‹ç±»å‹", "æœ€ä½³å‡†ç¡®ç‡", 
                "æœ€ç»ˆå‡†ç¡®ç‡", "å­¦ä¹ ç‡", "æ‰¹æ¬¡å¤§å°", "è®­ç»ƒè½®æ¬¡", "è€—æ—¶"
            ]
            logger.info(
                f"{headers[0]:<6} {headers[1]:<22} {headers[2]:<10} {headers[3]:<12} "
                f"{headers[4]:<12} {headers[5]:<8} {headers[6]:<8} {headers[7]:<8} {headers[8]:<10}"
            )
        else:
            headers = ["æ’å", "æ–‡ä»¶å", "æ¨¡å‹ç±»å‹", "æœ€ä½³å‡†ç¡®ç‡", "è®­ç»ƒè½®æ¬¡", "è·¯å¾„"]
            logger.info(
                f"{headers[0]:<6} {headers[1]:<40} {headers[2]:<10} {headers[3]:<12} "
                f"{headers[4]:<8} {headers[5]:<30}"
            )

        logger.info("-" * 120)

        # è¡¨æ ¼å†…å®¹ï¼ˆå¸¦æ’åæ ‡è®°ï¼‰
        for i, item in enumerate(display_items, 1):
            rank_mark = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else ""

            if is_run_summary:
                logger.info(
                    f"{i:<6} {os.path.basename(item['dir'])[:20]:<22} {item['model_type']:<10} "
                    f"{item['metrics']['best_acc']:.4f}    {item['metrics']['final_acc']:.4f}    "
                    f"{str(item['params']['lr']):<8} {str(item['params']['batch_size']):<8} "
                    f"{str(item['params']['epochs']):<8} {item['metrics']['time_cost']:<10} {rank_mark}"
                )
            else:
                # è·¯å¾„è¿‡é•¿æ—¶æˆªæ–­
                path_display = (
                    item["path"][:28] + "..."
                    if len(item["path"]) > 30
                    else item["path"]
                )
                logger.info(
                    f"{i:<6} {item['filename'][:38]:<40} {item['model_type']:<10} "
                    f"{item['metrics']['best_acc']:.4f}    {item['metrics']['epoch']:<8} "
                    f"{path_display:<30} {rank_mark}"
                )

        logger.info("=" * 120)

    @staticmethod
    def print_statistics(items: List[Dict[str, Any]]) -> None:
        """æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ€é«˜å‡†ç¡®ç‡ã€å¹³å‡å€¼ç­‰ï¼‰"""
        if not items:
            return

        valid_items = [item for item in items if "best_acc" in item["metrics"]]
        if not valid_items:
            logger.info("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯: æ— æœ‰æ•ˆå‡†ç¡®ç‡æ•°æ®")
            return

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        best_item = max(valid_items, key=lambda x: x["metrics"]["best_acc"])
        avg_acc = sum(item["metrics"]["best_acc"] for item in valid_items) / len(valid_items)
        acc_std = (sum((item["metrics"]["best_acc"] - avg_acc) **2 for item in valid_items)
                  / len(valid_items))** 0.5

        logger.info("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  â”œâ”€ æœ€é«˜å‡†ç¡®ç‡: {best_item['metrics']['best_acc']:.4f}")
        logger.info(f"  â”œâ”€ å¹³å‡æœ€ä½³å‡†ç¡®ç‡: {avg_acc:.4f}")
        logger.info(f"  â”œâ”€ å‡†ç¡®ç‡æ ‡å‡†å·®: {acc_std:.4f}")
        if items[0]["type"] == "run":
            logger.info(f"  â””â”€ æœ€é«˜å‡†ç¡®ç‡ç›®å½•: {os.path.basename(best_item['dir'])}")
        else:
            logger.info(f"  â””â”€ æœ€é«˜å‡†ç¡®ç‡æ¨¡å‹: {best_item['filename']}")


class ModelAnalysisService:
    """æ¨¡å‹åˆ†ææœåŠ¡ç±»ï¼šç»Ÿä¸€ç®¡ç†æ ¸å¿ƒåˆ†æåŠŸèƒ½"""

    @staticmethod
    def summarize_runs(
        run_dir_pattern: str = "run_*", top_n: int = 10, root_dir: str = "."
    ) -> List[Dict[str, Any]]:
        """æ±‡æ€»å¤šä¸ªè®­ç»ƒä»»åŠ¡çš„ç»“æœ"""
        logger.info(f"ğŸ“Š å¼€å§‹æ±‡æ€»è®­ç»ƒç»“æœ (æ¨¡å¼: {run_dir_pattern}, æ ¹ç›®å½•: {root_dir})")

        # æŸ¥æ‰¾åŒ¹é…ç›®å½•
        run_dirs = PathScanner.find_run_directories(run_dir_pattern, root_dir)
        if not run_dirs:
            logger.info(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{run_dir_pattern}' çš„è®­ç»ƒç›®å½•")
            return []

        logger.info(f"âœ… æ‰¾åˆ° {len(run_dirs)} ä¸ªåŒ¹é…çš„è®­ç»ƒç›®å½•")

        # æå–ç›®å½•ä¿¡æ¯
        run_infos = []
        for dir_path in run_dirs:
            info = ModelInfo.from_run_directory(dir_path)
            if info:
                run_infos.append(info)
            else:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆç›®å½•: {os.path.basename(dir_path)}")

        if not run_infos:
            logger.info("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒä¿¡æ¯å¯æ±‡æ€»")
            return []

        # æ’åºå¹¶å±•ç¤º
        sorted_runs = ResultVisualizer.sort_by_metric(run_infos)
        ResultVisualizer.print_summary_table(sorted_runs, top_n)
        ResultVisualizer.print_statistics(sorted_runs)

        return sorted_runs

    @staticmethod
    def compare_models_by_dir(
        dir_pattern: str = "run_*",
        root_dir: str = ".",
        top_n: int = 10,
        model_file_pattern: str = "*.pth",
    ) -> List[Dict[str, Any]]:
        """æŒ‰ç›®å½•æ¨¡å¼è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶å¹¶æ¯”è¾ƒ"""
        logger.info(
            f"ğŸ”„ å¼€å§‹æ¯”è¾ƒç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶ "
            f"(ç›®å½•æ¨¡å¼: {dir_pattern}, æ ¹ç›®å½•: {root_dir}, æ¨¡å‹è§„åˆ™: {model_file_pattern})"
        )

        # 1. æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„ç›®å½•
        target_dirs = PathScanner.find_run_directories(dir_pattern, root_dir)
        if not target_dirs:
            logger.info(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{dir_pattern}' çš„ç›®å½•ï¼ˆæ ¹ç›®å½•: {root_dir}ï¼‰")
            return []
        logger.info(f"âœ… æ‰¾åˆ° {len(target_dirs)} ä¸ªåŒ¹é…ç›®å½•")

        # 2. æ”¶é›†æ‰€æœ‰ç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶ï¼ˆå»é‡ï¼‰
        model_files = set()
        for dir_path in target_dirs:
            pth_files = PathScanner.find_model_files(dir_path, model_file_pattern)
            if pth_files:
                abs_pths = [os.path.abspath(pth) for pth in pth_files]
                model_files.update(abs_pths)
                logger.info(
                    f"  â”œâ”€ ç›®å½• {os.path.basename(dir_path)}: æ‰¾åˆ° {len(pth_files)} ä¸ªæ¨¡å‹æ–‡ä»¶"
                )
            else:
                logger.info(f"  â”œâ”€ ç›®å½• {os.path.basename(dir_path)}: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡")

        model_files_list = list(model_files)
        if not model_files_list:
            logger.info("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆæ¨¡å‹æ–‡ä»¶")
            return []
        logger.info(f"âœ… å…±æ”¶é›†åˆ° {len(model_files_list)} ä¸ªå”¯ä¸€æ¨¡å‹æ–‡ä»¶")

        # 3. æå–æ¨¡å‹ä¿¡æ¯å¹¶å±•ç¤º
        model_infos = []
        for pth_path in model_files_list:
            info = ModelInfo.from_model_file(pth_path)
            if info:
                model_infos.append(info)
            else:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆæ¨¡å‹: {os.path.basename(pth_path)}")

        if not model_infos:
            logger.info("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„æœ‰æ•ˆæ¨¡å‹ä¿¡æ¯")
            return []

        # 4. æ’åºå¹¶å±•ç¤º
        sorted_models = ResultVisualizer.sort_by_metric(model_infos)
        ResultVisualizer.print_summary_table(sorted_models, top_n)
        ResultVisualizer.print_statistics(sorted_models)

        return sorted_models

    @staticmethod
    def compare_latest_models(
        pattern: str = "run_*",  # ç›®å½•è¿‡æ»¤æ¨¡å¼
        num_latest: int = 5,  # å–æœ€æ–°Nä¸ªç›®å½•
        root_dir: str = ".",  # æ ¹æœç´¢ç›®å½•
    ) -> List[Dict[str, Any]]:
        """æ¯”è¾ƒæŒ‡å®šæ¨¡å¼ä¸‹æœ€æ–°Nä¸ªè®­ç»ƒç›®å½•ä¸­çš„æœ€ä½³æ¨¡å‹"""
        logger.info(
            f"ğŸ” æ¯”è¾ƒæœ€æ–°çš„ {num_latest} ä¸ªè®­ç»ƒç›®å½•ä¸­çš„æœ€ä½³æ¨¡å‹ "
            f"(ç›®å½•æ¨¡å¼: {pattern}, æ ¹ç›®å½•: {root_dir})"
        )

        # 1. æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„ç›®å½•ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´å€’åºï¼‰
        matched_dirs = PathScanner.find_run_directories(pattern, root_dir)
        if not matched_dirs:
            logger.info(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„è®­ç»ƒç›®å½•ï¼ˆæ ¹ç›®å½•: {root_dir}ï¼‰")
            return []

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„num_latestä¸ªç›®å½•
        sorted_dirs = sorted(matched_dirs, key=lambda x: os.path.getmtime(x), reverse=True)
        latest_dirs = sorted_dirs[:num_latest]

        if not latest_dirs:
            logger.info("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æœ€æ–°ç›®å½•")
            return []
        logger.info(f"âœ… æ‰¾åˆ°æœ€æ–°çš„ {len(latest_dirs)} ä¸ªç›®å½•")

        # 2. æå–æ¯ä¸ªç›®å½•çš„æœ€ä½³æ¨¡å‹ï¼ˆä¼˜å…ˆæ‰¾best_model*.pthï¼‰
        model_files = []
        for dir_path in latest_dirs:
            best_models = PathScanner.find_model_files(dir_path, "best_model*.pth")
            if best_models:
                # å–ç›®å½•ä¸­æœ€æ–°ä¿®æ”¹çš„æœ€ä½³æ¨¡å‹
                latest_model = max(best_models, key=lambda x: os.path.getmtime(x))
                model_files.append(latest_model)
                logger.info(
                    f"  â”œâ”€ ç›®å½• {os.path.basename(dir_path)}: æœ€æ–°æœ€ä½³æ¨¡å‹ {os.path.basename(latest_model)}"
                )
            else:
                logger.info(f"âš ï¸ ç›®å½• {os.path.basename(dir_path)}: æœªæ‰¾åˆ°best_model*.pthï¼Œè·³è¿‡")

        if not model_files:
            logger.info("âŒ æ²¡æœ‰æ‰¾åˆ°å¯æ¯”è¾ƒçš„æœ€ä½³æ¨¡å‹æ–‡ä»¶")
            return []

        # 3. æå–æ¨¡å‹ä¿¡æ¯å¹¶å±•ç¤º
        model_infos = []
        for model_path in model_files:
            info = ModelInfo.from_model_file(model_path)
            if info:
                model_infos.append(info)
            else:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆæ¨¡å‹æ–‡ä»¶: {os.path.basename(model_path)}")

        if not model_infos:
            logger.info("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„æœ‰æ•ˆæ¨¡å‹ä¿¡æ¯")
            return []

        # 4. æ’åºå¹¶å±•ç¤ºç»“æœ
        sorted_models = ResultVisualizer.sort_by_metric(model_infos)
        ResultVisualizer.print_summary_table(sorted_models, len(sorted_models))
        ResultVisualizer.print_statistics(sorted_models)

        return sorted_models

    @staticmethod
    def visualize_training_metrics(run_dir=None, metrics_path=None, root_dir="."):
        """é€šè¿‡æ³¨å†Œä¸­å¿ƒå®ç°è‡ªåŠ¨åŒ–å¯è§†åŒ–"""
        logger.info("\nğŸ¨ å¼€å§‹å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡...")

        # 1. ç¡®å®šç›®æ ‡æ–‡ä»¶
        target_files = []
        
        if metrics_path:
            # ä½¿ç”¨æŒ‡å®šçš„æŒ‡æ ‡æ–‡ä»¶
            if not os.path.isabs(metrics_path):
                metrics_path = os.path.join(root_dir, metrics_path)
            target_files = [metrics_path]
            logger.info(f"  â”œâ”€ ä½¿ç”¨æŒ‡å®šçš„æŒ‡æ ‡æ–‡ä»¶: {metrics_path}")
        elif run_dir:
            # ä½¿ç”¨æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡æ–‡ä»¶
            if not os.path.isabs(run_dir):
                run_dir = os.path.join(root_dir, run_dir)
            if not os.path.exists(run_dir):
                logger.error(f"  â””â”€ æŒ‡å®šçš„è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {run_dir}")
                return []
                
            target_files = PathScanner.find_metric_files(run_dir)
            logger.info(f"  â”œâ”€ ä»æŒ‡å®šç›®å½•åŠ è½½æŒ‡æ ‡æ–‡ä»¶: {run_dir} (æ‰¾åˆ° {len(target_files)} ä¸ª)")
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°è®­ç»ƒç›®å½•
            logger.info("  â”œâ”€ æœªæŒ‡å®šç›®å½•æˆ–æ–‡ä»¶ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°è®­ç»ƒç›®å½•...")
            latest_dir = PathScanner.get_latest_run_directory("run_*", root_dir)
            
            if not latest_dir:
                logger.info("  â””â”€ âŒ æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒç›®å½•ï¼ˆéœ€ä»¥'run_'å¼€å¤´ï¼‰")
                return []
                
            target_files = PathScanner.find_metric_files(latest_dir)
            logger.info(f"  â””â”€ è‡ªåŠ¨é€‰æ‹©æœ€æ–°ç›®å½•: {latest_dir} (æ‰¾åˆ° {len(target_files)} ä¸ªæŒ‡æ ‡æ–‡ä»¶)")

        if not target_files:
            logger.info("  â””â”€ âŒ æœªæ‰¾åˆ°ä»»ä½•æŒ‡æ ‡æ–‡ä»¶")
            return []

        # 2. æ‰¹é‡è§£ææŒ‡æ ‡ï¼ˆé€šè¿‡è§£æå™¨æ³¨å†Œä¸­å¿ƒï¼‰
        metric_datas = []
        for file in target_files:
            data = MetricParserRegistry.parse_file(file)
            if data:
                metric_datas.append(data)

        if not metric_datas:
            logger.info("  â””â”€ âŒ æ²¡æœ‰è§£ææˆåŠŸçš„æŒ‡æ ‡æ•°æ®")
            return []

        # 3. æ‰¹é‡å¯è§†åŒ–ï¼ˆé€šè¿‡å¯è§†åŒ–å™¨æ³¨å†Œä¸­å¿ƒï¼‰
        results = []
        for data in metric_datas:
            logger.info(f"  â”œâ”€ å¯è§†åŒ–æŒ‡æ ‡: {data.metric_type} (æ¥æº: {os.path.basename(data.source_path)})")
            vis_result = VisualizerRegistry.draw(data)
            if vis_result:
                results.append(vis_result)

        logger.info(f"  â””â”€ âœ… å®Œæˆå¯è§†åŒ–ï¼Œå…±å¤„ç† {len(results)} ä¸ªæŒ‡æ ‡")
        return results


# ==================================================
# ä¸»å‡½æ•°å…¥å£
# ==================================================
def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="æ¨¡å‹åˆ†æå·¥å…·ï¼šæ±‡æ€»è®­ç»ƒç»“æœã€æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ã€å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡"
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="summarize",
        choices=["summarize", "compare", "latest", "analyze"],
        help="è¿è¡Œæ¨¡å¼: "
        "summarize(æ±‡æ€»è®­ç»ƒç›®å½•), "
        "compare(æŒ‰ç›®å½•æŸ¥æ‰¾æ¨¡å‹å¹¶æ¯”è¾ƒ), "
        "latest(æ¯”è¾ƒæœ€æ–°Nä¸ªç›®å½•çš„æœ€ä½³æ¨¡å‹), "
        "analyze(å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡)",
    )
    parser.add_argument(
        "--pattern",
        type=str,
        default="runs/run_*",
        help="ç›®å½•åŒ¹é…æ¨¡å¼ï¼ˆsummarize/compare/latestæ¨¡å¼å‡ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=10,
        help="æ˜¾ç¤ºçš„ç»“æœæ•°é‡ï¼ˆsummarize/compareæ¨¡å¼ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--num-latest", type=int, default=5, help="å–æœ€æ–°çš„ç›®å½•æ•°é‡ï¼ˆä»…latestæ¨¡å¼ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--root-dir", type=str, default=".", help="æ ¹æœç´¢ç›®å½•ï¼ˆæ‰€æœ‰æ¨¡å¼å‡ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--run-dir", type=str, default=None, help="è®­ç»ƒç›®å½•è·¯å¾„ï¼ˆä»…analyzeæ¨¡å¼ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--metrics-path",
        type=str,
        default=None,
        help="æŒ‡æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆä»…analyzeæ¨¡å¼ç”Ÿæ•ˆï¼‰",
    )

    args = parser.parse_args()

    # æŒ‰æ¨¡å¼è°ƒç”¨ModelAnalysisServiceçš„å¯¹åº”é™æ€æ–¹æ³•
    if args.mode == "summarize":
        ModelAnalysisService.summarize_runs(args.pattern, args.top_n, args.root_dir)
    elif args.mode == "compare":
        ModelAnalysisService.compare_models_by_dir(
            dir_pattern=args.pattern, root_dir=args.root_dir, top_n=args.top_n
        )
    elif args.mode == "latest":
        ModelAnalysisService.compare_latest_models(
            pattern=args.pattern, num_latest=args.num_latest, root_dir=args.root_dir
        )
    elif args.mode == "analyze":
        ModelAnalysisService.visualize_training_metrics(
            run_dir=args.run_dir, metrics_path=args.metrics_path, root_dir=args.root_dir
        )


if __name__ == "__main__":
    main()
