import os
import json
import glob
import torch
import matplotlib.pyplot as plt
import sys
import numpy as np
from typing import List, Dict, Optional, Any, Callable, Type
from functools import lru_cache
from abc import ABC, abstractmethod
from dataclasses import dataclass

# è§£å†³OpenMPè¿è¡Œæ—¶åº“å†²çªé—®é¢˜
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# å¯¼å…¥è‡ªå®šä¹‰æ—¥å¿—æ¨¡å—
from src.utils.log_utils import get_logger

# åˆå§‹åŒ–æ—¥å¿—å™¨
logger = get_logger(name=__name__, log_file="logs/model_analysis.log", global_level="INFO")

# å¯¼å…¥å·¥å…·æ³¨å†Œä¸­å¿ƒ
from src.helper_utils.helper_tools_registry import ToolRegistry


# ==================================================
# æ ¸å¿ƒæ•°æ®æ¨¡å‹å±‚ï¼šæ ‡å‡†åŒ–æ•°æ®è¡¨ç¤ºï¼Œå¢å¼ºæ ¸å¿ƒæ•°æ®æ¨¡å‹åœ°ä½
# ==================================================
@dataclass
class MetricData:
    """æ ‡å‡†åŒ–æŒ‡æ ‡æ•°æ®æ¨¡å‹ï¼šç»Ÿä¸€ä¸åŒæŒ‡æ ‡æ–‡ä»¶çš„å†…å­˜è¡¨ç¤º"""
    metric_type: str  # æŒ‡æ ‡ç±»å‹ï¼š"epoch_curve" / "confusion_matrix" / "lr_curve" ç­‰
    data: Dict[str, Any]  # ç»“æ„åŒ–æŒ‡æ ‡æ•°æ®
    source_path: str  # æ•°æ®æ¥æºæ–‡ä»¶è·¯å¾„
    timestamp: float  # æ•°æ®ç”Ÿæˆæ—¶é—´æˆ³

@dataclass
class ModelInfoData:
    """æ¨¡å‹ä¿¡æ¯æ•°æ®æ¨¡å‹ï¼šç»Ÿä¸€æ¨¡å‹å’Œè®­ç»ƒä»»åŠ¡çš„ä¿¡æ¯è¡¨ç¤º"""
    type: str  # "run" æˆ– "model"
    path: str  # è·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶ï¼‰
    model_type: str  # æ¨¡å‹ç±»å‹
    params: Dict[str, Any]  # æ¨¡å‹å‚æ•°
    metrics: Dict[str, Any]  # æ€§èƒ½æŒ‡æ ‡
    timestamp: float  # æ—¶é—´æˆ³


# ==================================================
# æ•°æ®è®¿é—®å±‚ï¼šè´Ÿè´£æ•°æ®çš„è¯»å–å’Œè§£æï¼Œæä¾›ç»Ÿä¸€çš„è®¿é—®æ¥å£
# ==================================================
class DataAccessor:
    """æ•°æ®è®¿é—®å™¨ï¼šæä¾›ç»Ÿä¸€çš„æ•°æ®è¯»å–å’Œè§£ææ¥å£"""
    
    @staticmethod
    def read_file(file_path: str) -> Optional[Any]:
        """é€šç”¨æ–‡ä»¶è¯»å–æ–¹æ³•"""
        if not os.path.exists(file_path):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        try:
            if file_path.endswith('.json'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            elif file_path.endswith('.pth'):
                # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒweights_onlyå‚æ•°
                try:
                    return torch.load(file_path, map_location="cpu", weights_only=True)
                except TypeError:
                    return torch.load(file_path, map_location="cpu")
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                return None
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None

    @staticmethod
    def get_file_timestamp(file_path: str) -> float:
        """è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æˆ³"""
        if os.path.exists(file_path):
            return os.path.getmtime(file_path)
        return 0.0


class PathScanner:
    """è·¯å¾„æ‰«ææ¨¡å—ï¼šè´Ÿè´£æŸ¥æ‰¾è®­ç»ƒç›®å½•å’Œå„ç±»æ–‡ä»¶"""
    
    @staticmethod
    def find_run_directories(
        pattern: str = "run_\*", root_dir: str = "."  # æ”¯æŒè‡ªå®šä¹‰æ ¹ç›®å½•
    ) -> List[str]:
        """æ ¹æ®æ¨¡å¼æŸ¥æ‰¾è®­ç»ƒç›®å½•"""
        dir_pattern = os.path.join(root_dir, pattern)
        all_entries = glob.glob(dir_pattern)
        return [entry for entry in all_entries if os.path.isdir(entry)]

    @staticmethod
    def find_model_files(directory: str, pattern: str = "\*.pth") -> List[str]:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶ï¼ˆéé€’å½’ï¼‰"""
        if not os.path.exists(directory):
            return []
        return glob.glob(os.path.join(directory, pattern))

    @staticmethod
    def find_metric_files(directory: str, pattern: str = "\*.json") -> List[str]:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æŒ‡æ ‡æ–‡ä»¶ï¼ˆéé€’å½’ï¼‰"""
        if not os.path.exists(directory):
            return []
        return glob.glob(os.path.join(directory, pattern))

    @staticmethod
    def get_latest_run_directory(
        pattern: str = "run_\*", root_dir: str = "."
    ) -> Optional[str]:
        """è·å–æœ€æ–°ä¿®æ”¹çš„è®­ç»ƒç›®å½•"""
        run_dirs = PathScanner.find_run_directories(pattern, root_dir)
        if not run_dirs:
            return None
        return max(run_dirs, key=lambda x: os.path.getmtime(x))


# ==================================================
# è§£æå™¨å±‚ï¼šè´Ÿè´£è§£æä¸åŒæ ¼å¼çš„æŒ‡æ ‡æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ•°æ®æ¨¡å‹
# ==================================================
class BaseMetricParser(ABC):
    """æŒ‡æ ‡è§£æå™¨æŠ½è±¡æ¥å£ï¼šå®šä¹‰æ‰€æœ‰è§£æå™¨çš„ç»Ÿä¸€è§„èŒƒ"""
    @abstractmethod
    def support(self, file_path: str) -> bool:
        """åˆ¤æ–­å½“å‰è§£æå™¨æ˜¯å¦æ”¯æŒè¯¥æ–‡ä»¶"""
        pass

    @abstractmethod
    def parse(self, file_path: str) -> Optional[MetricData]:
        """è§£ææ–‡ä»¶ä¸ºæ ‡å‡†åŒ–MetricData"""
        pass


class MetricParserRegistry:
    """æŒ‡æ ‡è§£æå™¨æ³¨å†Œä¸­å¿ƒï¼šç®¡ç†æ‰€æœ‰è§£æå™¨ï¼Œå®ç°è‡ªåŠ¨åŒ¹é…"""
    _parsers: List[BaseMetricParser] = []

    @classmethod
    def register(cls, parser: BaseMetricParser):
        """æ³¨å†Œè§£æå™¨å®ä¾‹"""
        cls._parsers.append(parser)

    @classmethod
    def get_matched_parser(cls, file_path: str) -> Optional[BaseMetricParser]:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„åŒ¹é…æœ€åˆé€‚çš„è§£æå™¨"""
        for parser in cls._parsers:
            if parser.support(file_path):
                return parser
        return None

    @classmethod
    def parse_file(cls, file_path: str) -> Optional[MetricData]:
        """è‡ªåŠ¨è§£ææ–‡ä»¶çš„å…¥å£æ–¹æ³•"""
        parser = cls.get_matched_parser(file_path)
        if not parser:
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„è§£æå™¨: {file_path}")
            return None
        
        try:
            return parser.parse(file_path)
        except Exception as e:
            logger.error(f"è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None


# ==================================================
# å…·ä½“è§£æå™¨å®ç°
# ==================================================
class EpochMetricsParser(BaseMetricParser):
    """epoch_metrics.jsonè§£æå™¨ï¼šå¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„losså’Œaccæ›²çº¿æ•°æ®"""
    def support(self, file_path: str) -> bool:
        return file_path.endswith("epoch_metrics.json")

    def parse(self, file_path: str) -> Optional[MetricData]:
        try:
            raw_data = DataAccessor.read_file(file_path)
            if not raw_data:
                return None
            
            return MetricData(
                metric_type="epoch_curve",
                data={
                    "epochs": [item["epoch"] for item in raw_data],
                    "train_loss": [item["train_loss"] for item in raw_data],
                    "train_acc": [item["train_acc"] for item in raw_data],
                    "test_acc": [item["test_acc"] for item in raw_data],
                },
                source_path=file_path,
                timestamp=DataAccessor.get_file_timestamp(file_path)
            )
        except Exception as e:
            logger.error(f"è§£æepochæŒ‡æ ‡æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None


class FullMetricsParser(BaseMetricParser):
    """metrics.jsonè§£æå™¨ï¼šå¤„ç†åŒ…å«å®Œæ•´è®­ç»ƒä¿¡æ¯çš„æŒ‡æ ‡æ–‡ä»¶"""
    def support(self, file_path: str) -> bool:
        return file_path.endswith("metrics.json") and not file_path.endswith("epoch_metrics.json")

    def parse(self, file_path: str) -> Optional[MetricData]:
        try:
            raw_data = DataAccessor.read_file(file_path)
            if not raw_data or "epoch_metrics" not in raw_data:
                return None
            
            epoch_data = raw_data["epoch_metrics"]
            return MetricData(
                metric_type="epoch_curve",
                data={
                    "epochs": [item["epoch"] for item in epoch_data],
                    "train_loss": [item["train_loss"] for item in epoch_data],
                    "train_acc": [item["train_acc"] for item in epoch_data],
                    "test_acc": [item["test_acc"] for item in epoch_data],
                },
                source_path=file_path,
                timestamp=DataAccessor.get_file_timestamp(file_path)
            )
        except Exception as e:
            logger.error(f"è§£æå®Œæ•´æŒ‡æ ‡æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None


class ConfusionMatrixParser(BaseMetricParser):
    """æ··æ·†çŸ©é˜µè§£æå™¨ï¼šå¤„ç†confusion_matrix.jsonæ–‡ä»¶"""
    def support(self, file_path: str) -> bool:
        return file_path.endswith("confusion_matrix.json")

    def parse(self, file_path: str) -> Optional[MetricData]:
        try:
            raw_data = DataAccessor.read_file(file_path)
            if not raw_data:
                return None
            
            return MetricData(
                metric_type="confusion_matrix",
                data={
                    "classes": raw_data.get("classes", []),
                    "matrix": raw_data.get("matrix", []),
                    "accuracy": raw_data.get("accuracy", 0.0)
                },
                source_path=file_path,
                timestamp=DataAccessor.get_file_timestamp(file_path)
            )
        except Exception as e:
            logger.error(f"è§£ææ··æ·†çŸ©é˜µæ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None


# ==================================================
# å¯è§†åŒ–å±‚ï¼šè´Ÿè´£å°†æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹è½¬æ¢ä¸ºå¯è§†åŒ–ç»“æœ
# ==================================================
class BaseVisualizer(ABC):
    """å¯è§†åŒ–å™¨æŠ½è±¡æ¥å£ï¼šå®šä¹‰æ‰€æœ‰å¯è§†åŒ–å™¨çš„ç»Ÿä¸€è§„èŒƒ"""
    @abstractmethod
    def support(self, metric_data: MetricData) -> bool:
        """åˆ¤æ–­å½“å‰å¯è§†åŒ–å™¨æ˜¯å¦æ”¯æŒè¯¥æŒ‡æ ‡æ•°æ®"""
        pass

    @abstractmethod
    def visualize(self, metric_data: MetricData, show: bool = True) -> Any:
        """ç»˜åˆ¶å¯è§†åŒ–ç»“æœï¼ˆè¿”å›ç»˜å›¾å¯¹è±¡ï¼‰"""
        pass


class VisualizerRegistry:
    """å¯è§†åŒ–å™¨æ³¨å†Œä¸­å¿ƒï¼šç®¡ç†æ‰€æœ‰å¯è§†åŒ–å™¨ï¼Œå®ç°è‡ªåŠ¨åŒ¹é…"""
    _visualizers: List[BaseVisualizer] = []

    @classmethod
    def register(cls, visualizer: BaseVisualizer):
        """æ³¨å†Œå¯è§†åŒ–å™¨å®ä¾‹"""
        cls._visualizers.append(visualizer)

    @classmethod
    def get_matched_visualizer(cls, metric_data: MetricData) -> Optional[BaseVisualizer]:
        """æ ¹æ®æŒ‡æ ‡æ•°æ®åŒ¹é…æœ€åˆé€‚çš„å¯è§†åŒ–å™¨"""
        for vis in cls._visualizers:
            if vis.support(metric_data):
                return vis
        return None

    @classmethod
    def draw(cls, metric_data: MetricData, show: bool = True) -> Any:
        """è‡ªåŠ¨ç»˜åˆ¶å¯è§†åŒ–ç»“æœçš„å…¥å£æ–¹æ³•"""
        vis = cls.get_matched_visualizer(metric_data)
        if not vis:
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„å¯è§†åŒ–å™¨: {metric_data.metric_type}")
            return None
            
        try:
            return vis.visualize(metric_data, show)
        except Exception as e:
            logger.error(f"å¯è§†åŒ–å¤±è´¥ {metric_data.source_path}: {str(e)}")
            return None


# ==================================================
# å…·ä½“å¯è§†åŒ–å™¨å®ç°
# ==================================================
class CurveVisualizer(BaseVisualizer):
    """æ›²çº¿å¯è§†åŒ–å™¨ï¼šå±•ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„losså’Œaccå˜åŒ–æ›²çº¿"""
    def support(self, metric_data: MetricData) -> bool:
        return metric_data.metric_type == "epoch_curve"

    def visualize(self, metric_data: MetricData, show: bool = True) -> Any:
        try:
            from d2l import torch as d2l  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…ä¸å¿…è¦çš„ä¾èµ–åŠ è½½
            
            # ä½¿ç”¨å·¥å…·æ³¨å†Œä¸­å¿ƒè®¾ç½®ä¸­æ–‡å­—ä½“
            ToolRegistry.call("setup_font")
            data = metric_data.data
            
            animator = d2l.Animator(
                xlabel="è¿­ä»£å‘¨æœŸ",
                xlim=[1, len(data["epochs"])],
                legend=["è®­ç»ƒæŸå¤±", "è®­ç»ƒå‡†ç¡®ç‡", "æµ‹è¯•å‡†ç¡®ç‡"],
                title=f"è®­ç»ƒæ›²çº¿ (æ¥æº: {os.path.basename(metric_data.source_path)})"
            )
            
            for i in range(len(data["epochs"])):
                animator.add(
                    data["epochs"][i],
                    (data["train_loss"][i], data["train_acc"][i], data["test_acc"][i])
                )
            
            if show:
                plt.show()
            return animator
        except Exception as e:
            logger.error(f"ç»˜åˆ¶æ›²çº¿å¯è§†åŒ–å¤±è´¥: {str(e)}")
            return None


class ConfusionMatrixVisualizer(BaseVisualizer):
    """æ··æ·†çŸ©é˜µå¯è§†åŒ–å™¨ï¼šå±•ç¤ºæ¨¡å‹åˆ†ç±»ç»“æœçš„æ··æ·†çŸ©é˜µ"""
    def support(self, metric_data: MetricData) -> bool:
        return metric_data.metric_type == "confusion_matrix"

    def visualize(self, metric_data: MetricData, show: bool = True) -> plt.Figure:
        try:
            # ä½¿ç”¨å·¥å…·æ³¨å†Œä¸­å¿ƒè®¾ç½®ä¸­æ–‡å­—ä½“
            ToolRegistry.call("setup_font")
            data = metric_data.data
            matrix = np.array(data["matrix"])
            classes = data["classes"]
            
            # ç¡®ä¿çŸ©é˜µå’Œç±»åˆ«æ•°é‡åŒ¹é…
            if len(matrix) != len(classes) or len(matrix[0]) != len(classes):
                logger.error("æ··æ·†çŸ©é˜µç»´åº¦ä¸ç±»åˆ«æ•°é‡ä¸åŒ¹é…")
                return None
                
            # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            fig, ax = plt.subplots(figsize=(8, 6))
            im = ax.imshow(matrix, cmap="Blues")
            
            # è®¾ç½®åæ ‡è½´
            ax.set_xticks(range(len(classes)))
            ax.set_yticks(range(len(classes)))
            ax.set_xticklabels(classes)
            ax.set_yticklabels(classes)
            ax.set_xlabel("é¢„æµ‹ç±»åˆ«")
            ax.set_ylabel("çœŸå®ç±»åˆ«")
            ax.set_title(f"æ··æ·†çŸ©é˜µ (å‡†ç¡®ç‡: {data['accuracy']:.4f})")
            
            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for i in range(len(classes)):
                for j in range(len(classes)):
                    ax.text(j, i, str(matrix[i, j]), ha="center", va="center")
            
            plt.colorbar(im)
            
            if show:
                plt.show()
            return fig
        except Exception as e:
            logger.error(f"ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯è§†åŒ–å¤±è´¥: {str(e)}")
            return None


# ==================================================
# ä¸šåŠ¡é€»è¾‘å±‚ï¼šæ•´åˆå„æ¨¡å—åŠŸèƒ½ï¼Œæä¾›å®Œæ•´çš„ä¸šåŠ¡é€»è¾‘å¤„ç†
# ==================================================
class ModelDataProcessor:
    """æ¨¡å‹æ•°æ®å¤„ç†å™¨ï¼šè´Ÿè´£å¤„ç†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘"""
    
    @staticmethod
    def extract_run_metrics(run_dir: str) -> Optional[Dict[str, Any]]:
        """ä»metrics.jsonæå–è®­ç»ƒæŒ‡æ ‡"""
        metrics_path = os.path.join(run_dir, "metrics.json")
        metrics = DataAccessor.read_file(metrics_path)
        
        if not metrics:
            return None

        return {
            "best_acc": float(metrics.get("best_test_acc", 0.0)),
            "final_acc": float(metrics.get("final_test_acc", 0.0)),
            "time_cost": metrics.get("total_training_time", "N/A"),
        }

    @staticmethod
    def extract_model_metrics(checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æ¨¡å‹æ£€æŸ¥ç‚¹æå–æŒ‡æ ‡ï¼ˆå…¼å®¹ä¸åŒæ ¼å¼ï¼‰"""
        return {
            "best_acc": float(
                checkpoint.get("best_test_acc", checkpoint.get("test_acc", 0.0))
            ),
            "epoch": int(checkpoint.get("epoch", 0)),
            "device": checkpoint.get("device", "N/A"),
        }

    @staticmethod
    def load_run_config(run_dir: str) -> Optional[Dict[str, Any]]:
        """ä»è®­ç»ƒç›®å½•åŠ è½½config.json"""
        config_path = os.path.join(run_dir, "config.json")
        return DataAccessor.read_file(config_path)

    @staticmethod
    def get_model_type(config: Optional[Dict[str, Any]]) -> str:
        """ä»é…ç½®ä¸­æå–æ¨¡å‹ç±»å‹"""
        if not config:
            return "Unknown"
        return config.get("model_name", "Unknown")

    @staticmethod
    def create_run_info(run_dir: str) -> Optional[ModelInfoData]:
        """ä»è®­ç»ƒç›®å½•ç”Ÿæˆä»»åŠ¡çº§ä¿¡æ¯"""
        config = ModelDataProcessor.load_run_config(run_dir)
        metrics = ModelDataProcessor.extract_run_metrics(run_dir)

        if not config or not metrics:
            return None

        return ModelInfoData(
            type="run",
            path=run_dir,
            model_type=ModelDataProcessor.get_model_type(config),
            params={
                "lr": config.get("learning_rate"),
                "batch_size": config.get("batch_size"),
                "epochs": config.get("num_epochs"),
                "timestamp": config.get("timestamp"),
            },
            metrics=metrics,
            timestamp=DataAccessor.get_file_timestamp(run_dir)
        )

    @staticmethod
    def create_model_info(model_path: str) -> Optional[ModelInfoData]:
        """ä»æ¨¡å‹æ–‡ä»¶ç”Ÿæˆå¿«ç…§çº§ä¿¡æ¯"""
        checkpoint = DataAccessor.read_file(model_path)
        if not checkpoint:
            return None

        # å°è¯•ä»æ¨¡å‹æ‰€åœ¨ç›®å½•åŠ è½½é…ç½®
        model_dir = os.path.dirname(model_path)
        config = ModelDataProcessor.load_run_config(model_dir)

        return ModelInfoData(
            type="model",
            path=model_path,
            model_type=ModelDataProcessor.get_model_type(config),
            params={
                "lr": config.get("learning_rate") if config else None,
                "batch_size": config.get("batch_size") if config else None,
            },
            metrics=ModelDataProcessor.extract_model_metrics(checkpoint),
            timestamp=DataAccessor.get_file_timestamp(model_path)
        )


class ResultVisualizer:
    """ç»“æœå±•ç¤ºæ¨¡å—ï¼šè´Ÿè´£æ’åºå’Œæ ¼å¼åŒ–è¾“å‡ºç»“æœ"""

    @staticmethod
    def sort_by_metric(
        items: List[ModelInfoData], metric_key: str = "best_acc", reverse: bool = True
    ) -> List[ModelInfoData]:
        """æŒ‰æŒ‡å®šæŒ‡æ ‡æ’åºï¼ˆé»˜è®¤æŒ‰æœ€ä½³å‡†ç¡®ç‡é™åºï¼‰"""
        valid_items = [item for item in items if metric_key in item.metrics]
        return sorted(
            valid_items, key=lambda x: x.metrics[metric_key], reverse=reverse
        )

    @staticmethod
    def print_summary_table(items: List[ModelInfoData], top_n: int = 10) -> None:
        """æ‰“å°æ ¼å¼åŒ–æ±‡æ€»è¡¨æ ¼ï¼ˆé€‚é…ä»»åŠ¡çº§/æ¨¡å‹çº§ä¿¡æ¯ï¼‰"""
        if not items:
            logger.info("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å±•ç¤º")
            return

        is_run_summary = items[0].type == "run"
        display_items = items[:top_n]

        # è¡¨æ ¼æ ‡é¢˜
        logger.info("\n" + "=" * 120)
        logger.info(f"ğŸ“Š åˆ†æç»“æœæ±‡æ€»ï¼ˆå…± {len(items)} é¡¹ï¼Œæ˜¾ç¤ºå‰ {len(display_items)} é¡¹ï¼‰")
        logger.info("=" * 120)

        # è¡¨å¤´ï¼ˆæŒ‰ä¿¡æ¯ç±»å‹åŒºåˆ†ï¼‰
        if is_run_summary:
            headers = [
                "æ’å", "ç›®å½•å", "æ¨¡å‹ç±»å‹", "æœ€ä½³å‡†ç¡®ç‡", 
                "æœ€ç»ˆå‡†ç¡®ç‡", "å­¦ä¹ ç‡", "æ‰¹æ¬¡å¤§å°", "è®­ç»ƒè½®æ¬¡", "è€—æ—¶"
            ]
            logger.info(
                f"{headers[0]:<6} {headers[1]:<22} {headers[2]:<10} {headers[3]:<12} "
                f"{headers[4]:<12} {headers[5]:<8} {headers[6]:<8} {headers[7]:<8} {headers[8]:<10}"
            )
        else:
            headers = ["æ’å", "æ–‡ä»¶å", "æ¨¡å‹ç±»å‹", "æœ€ä½³å‡†ç¡®ç‡", "è®­ç»ƒè½®æ¬¡", "è·¯å¾„"]
            logger.info(
                f"{headers[0]:<6} {headers[1]:<40} {headers[2]:<10} {headers[3]:<12} "
                f"{headers[4]:<8} {headers[5]:<30}"
            )

        logger.info("-" * 120)

        # è¡¨æ ¼å†…å®¹ï¼ˆå¸¦æ’åæ ‡è®°ï¼‰
        for i, item in enumerate(display_items, 1):
            rank_mark = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else ""

            if is_run_summary:
                logger.info(
                    f"{i:<6} {os.path.basename(item.path)[:20]:<22} {item.model_type:<10} "
                    f"{item.metrics['best_acc']:.4f}    {item.metrics['final_acc']:.4f}    "
                    f"{str(item.params['lr']):<8} {str(item.params['batch_size']):<8} "
                    f"{str(item.params['epochs']):<8} {item.metrics['time_cost']:<10} {rank_mark}"
                )
            else:
                # è·¯å¾„è¿‡é•¿æ—¶æˆªæ–­
                path_display = (
                    item.path[:28] + "..."
                    if len(item.path) > 30
                    else item.path
                )
                filename = os.path.basename(item.path)
                logger.info(
                    f"{i:<6} {filename[:38]:<40} {item.model_type:<10} "
                    f"{item.metrics['best_acc']:.4f}    {item.metrics['epoch']:<8} "
                    f"{path_display:<30} {rank_mark}"
                )

        logger.info("=" * 120)

    @staticmethod
    def print_statistics(items: List[ModelInfoData]) -> None:
        """æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ€é«˜å‡†ç¡®ç‡ã€å¹³å‡å€¼ç­‰ï¼‰"""
        if not items:
            return

        valid_items = [item for item in items if "best_acc" in item.metrics]
        if not valid_items:
            logger.info("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯: æ— æœ‰æ•ˆå‡†ç¡®ç‡æ•°æ®")
            return

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        best_item = max(valid_items, key=lambda x: x.metrics["best_acc"])
        avg_acc = sum(item.metrics["best_acc"] for item in valid_items) / len(valid_items)
        acc_std = (sum((item.metrics["best_acc"] - avg_acc) **2 for item in valid_items)
                  / len(valid_items))** 0.5

        logger.info("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  â”œâ”€ æœ€é«˜å‡†ç¡®ç‡: {best_item.metrics['best_acc']:.4f}")
        logger.info(f"  â”œâ”€ å¹³å‡æœ€ä½³å‡†ç¡®ç‡: {avg_acc:.4f}")
        logger.info(f"  â”œâ”€ å‡†ç¡®ç‡æ ‡å‡†å·®: {acc_std:.4f}")
        if items[0].type == "run":
            logger.info(f"  â””â”€ æœ€é«˜å‡†ç¡®ç‡ç›®å½•: {os.path.basename(best_item.path)}")
        else:
            logger.info(f"  â””â”€ æœ€é«˜å‡†ç¡®ç‡æ¨¡å‹: {os.path.basename(best_item.path)}")


# ==================================================
# æœåŠ¡ç¼–æ’å±‚ï¼šæ•´åˆå„æ¨¡å—æä¾›å®Œæ•´åˆ†ææœåŠ¡
# ==================================================
class ModelAnalysisService:
    """æ¨¡å‹åˆ†ææœåŠ¡ç±»ï¼šç»Ÿä¸€ç®¡ç†æ ¸å¿ƒåˆ†æåŠŸèƒ½"""

    @staticmethod
    def summarize_runs(
        run_dir_pattern: str = "run_\*", top_n: int = 10, root_dir: str = "."
    ) -> List[ModelInfoData]:
        """æ±‡æ€»å¤šä¸ªè®­ç»ƒä»»åŠ¡çš„ç»“æœ"""
        logger.info(f"ğŸ“Š å¼€å§‹æ±‡æ€»è®­ç»ƒç»“æœ (æ¨¡å¼: {run_dir_pattern}, æ ¹ç›®å½•: {root_dir})")

        # æŸ¥æ‰¾åŒ¹é…ç›®å½•
        run_dirs = PathScanner.find_run_directories(run_dir_pattern, root_dir)
        if not run_dirs:
            logger.info(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{run_dir_pattern}' çš„è®­ç»ƒç›®å½•")
            return []

        logger.info(f"âœ… æ‰¾åˆ° {len(run_dirs)} ä¸ªåŒ¹é…çš„è®­ç»ƒç›®å½•")

        # æå–ç›®å½•ä¿¡æ¯
        run_infos = []
        for dir_path in run_dirs:
            info = ModelDataProcessor.create_run_info(dir_path)
            if info:
                run_infos.append(info)
            else:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆç›®å½•: {os.path.basename(dir_path)}")

        if not run_infos:
            logger.info("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒä¿¡æ¯å¯æ±‡æ€»")
            return []

        # æ’åºå¹¶å±•ç¤º
        sorted_runs = ResultVisualizer.sort_by_metric(run_infos)
        ResultVisualizer.print_summary_table(sorted_runs, top_n)
        ResultVisualizer.print_statistics(sorted_runs)

        return sorted_runs

    @staticmethod
    def compare_models_by_dir(
        dir_pattern: str = "run_\*",
        root_dir: str = ".",
        top_n: int = 10,
        model_file_pattern: str = "\*.pth",
    ) -> List[ModelInfoData]:
        """æŒ‰ç›®å½•æ¨¡å¼è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶å¹¶æ¯”è¾ƒ"""
        logger.info(
            f"ğŸ”„ å¼€å§‹æ¯”è¾ƒç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶ "
            f"(ç›®å½•æ¨¡å¼: {dir_pattern}, æ ¹ç›®å½•: {root_dir}, æ¨¡å‹è§„åˆ™: {model_file_pattern})"
        )

        # 1. æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„ç›®å½•
        target_dirs = PathScanner.find_run_directories(dir_pattern, root_dir)
        if not target_dirs:
            logger.info(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{dir_pattern}' çš„ç›®å½•ï¼ˆæ ¹ç›®å½•: {root_dir}ï¼‰")
            return []
        logger.info(f"âœ… æ‰¾åˆ° {len(target_dirs)} ä¸ªåŒ¹é…ç›®å½•")

        # 2. æ”¶é›†æ‰€æœ‰ç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶ï¼ˆå»é‡ï¼‰
        model_files = set()
        for dir_path in target_dirs:
            pth_files = PathScanner.find_model_files(dir_path, model_file_pattern)
            if pth_files:
                abs_pths = [os.path.abspath(pth) for pth in pth_files]
                model_files.update(abs_pths)
                logger.info(
                    f"  â”œâ”€ ç›®å½• {os.path.basename(dir_path)}: æ‰¾åˆ° {len(pth_files)} ä¸ªæ¨¡å‹æ–‡ä»¶"
                )
            else:
                logger.info(f"  â”œâ”€ ç›®å½• {os.path.basename(dir_path)}: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡")

        model_files_list = list(model_files)
        if not model_files_list:
            logger.info("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆæ¨¡å‹æ–‡ä»¶")
            return []
        logger.info(f"âœ… å…±æ”¶é›†åˆ° {len(model_files_list)} ä¸ªå”¯ä¸€æ¨¡å‹æ–‡ä»¶")

        # 3. æå–æ¨¡å‹ä¿¡æ¯å¹¶å±•ç¤º
        model_infos = []
        for pth_path in model_files_list:
            info = ModelDataProcessor.create_model_info(pth_path)
            if info:
                model_infos.append(info)
            else:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆæ¨¡å‹: {os.path.basename(pth_path)}")

        if not model_infos:
            logger.info("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„æœ‰æ•ˆæ¨¡å‹ä¿¡æ¯")
            return []

        # 4. æ’åºå¹¶å±•ç¤º
        sorted_models = ResultVisualizer.sort_by_metric(model_infos)
        ResultVisualizer.print_summary_table(sorted_models, top_n)
        ResultVisualizer.print_statistics(sorted_models)

        return sorted_models

    @staticmethod
    def compare_latest_models(
        pattern: str = "run_\*",  # ç›®å½•è¿‡æ»¤æ¨¡å¼
        num_latest: int = 5,  # å–æœ€æ–°Nä¸ªç›®å½•
        root_dir: str = ".",  # æ ¹æœç´¢ç›®å½•
    ) -> List[ModelInfoData]:
        """æ¯”è¾ƒæŒ‡å®šæ¨¡å¼ä¸‹æœ€æ–°Nä¸ªè®­ç»ƒç›®å½•ä¸­çš„æœ€ä½³æ¨¡å‹"""
        logger.info(
            f"ğŸ” æ¯”è¾ƒæœ€æ–°çš„ {num_latest} ä¸ªè®­ç»ƒç›®å½•ä¸­çš„æœ€ä½³æ¨¡å‹ "
            f"(ç›®å½•æ¨¡å¼: {pattern}, æ ¹ç›®å½•: {root_dir})"
        )

        # 1. æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„ç›®å½•ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´å€’åºï¼‰
        matched_dirs = PathScanner.find_run_directories(pattern, root_dir)
        if not matched_dirs:
            logger.info(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„è®­ç»ƒç›®å½•ï¼ˆæ ¹ç›®å½•: {root_dir}ï¼‰")
            return []

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„num_latestä¸ªç›®å½•
        sorted_dirs = sorted(matched_dirs, key=lambda x: os.path.getmtime(x), reverse=True)
        latest_dirs = sorted_dirs[:num_latest]

        if not latest_dirs:
            logger.info("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æœ€æ–°ç›®å½•")
            return []
        logger.info(f"âœ… æ‰¾åˆ°æœ€æ–°çš„ {len(latest_dirs)} ä¸ªç›®å½•")

        # 2. æå–æ¯ä¸ªç›®å½•çš„æœ€ä½³æ¨¡å‹ï¼ˆä¼˜å…ˆæ‰¾best_model*.pthï¼‰
        model_files = []
        for dir_path in latest_dirs:
            best_models = PathScanner.find_model_files(dir_path, "best_model\*.pth")
            if best_models:
                # å–ç›®å½•ä¸­æœ€æ–°ä¿®æ”¹çš„æœ€ä½³æ¨¡å‹
                latest_model = max(best_models, key=lambda x: os.path.getmtime(x))
                model_files.append(latest_model)
                logger.info(
                    f"  â”œâ”€ ç›®å½• {os.path.basename(dir_path)}: æœ€æ–°æœ€ä½³æ¨¡å‹ {os.path.basename(latest_model)}"
                )
            else:
                logger.info(f"âš ï¸ ç›®å½• {os.path.basename(dir_path)}: æœªæ‰¾åˆ°best_model*.pthï¼Œè·³è¿‡")

        if not model_files:
            logger.info("âŒ æ²¡æœ‰æ‰¾åˆ°å¯æ¯”è¾ƒçš„æœ€ä½³æ¨¡å‹æ–‡ä»¶")
            return []

        # 3. æå–æ¨¡å‹ä¿¡æ¯å¹¶å±•ç¤º
        model_infos = []
        for model_path in model_files:
            info = ModelDataProcessor.create_model_info(model_path)
            if info:
                model_infos.append(info)
            else:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆæ¨¡å‹æ–‡ä»¶: {os.path.basename(model_path)}")

        if not model_infos:
            logger.info("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„æœ‰æ•ˆæ¨¡å‹ä¿¡æ¯")
            return []

        # 4. æ’åºå¹¶å±•ç¤ºç»“æœ
        sorted_models = ResultVisualizer.sort_by_metric(model_infos)
        ResultVisualizer.print_summary_table(sorted_models, len(sorted_models))
        ResultVisualizer.print_statistics(sorted_models)

        return sorted_models

    @staticmethod
    def visualize_training_metrics(run_dir=None, metrics_path=None, root_dir="."):
        """é€šè¿‡æ³¨å†Œä¸­å¿ƒå®ç°è‡ªåŠ¨åŒ–å¯è§†åŒ–"""
        logger.info("\nğŸ¨ å¼€å§‹å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡...")

        # 1. ç¡®å®šç›®æ ‡æ–‡ä»¶
        target_files = []
        
        if metrics_path:
            # ä½¿ç”¨æŒ‡å®šçš„æŒ‡æ ‡æ–‡ä»¶
            if not os.path.isabs(metrics_path):
                metrics_path = os.path.join(root_dir, metrics_path)
            target_files = [metrics_path]
            logger.info(f"  â”œâ”€ ä½¿ç”¨æŒ‡å®šçš„æŒ‡æ ‡æ–‡ä»¶: {metrics_path}")
        elif run_dir:
            # ä½¿ç”¨æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡æ–‡ä»¶
            if not os.path.isabs(run_dir):
                run_dir = os.path.join(root_dir, run_dir)
            if not os.path.exists(run_dir):
                logger.error(f"  â””â”€ æŒ‡å®šçš„è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {run_dir}")
                return []
                
            target_files = PathScanner.find_metric_files(run_dir)
            logger.info(f"  â”œâ”€ ä»æŒ‡å®šç›®å½•åŠ è½½æŒ‡æ ‡æ–‡ä»¶: {run_dir} (æ‰¾åˆ° {len(target_files)} ä¸ª)")
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°è®­ç»ƒç›®å½•
            logger.info("  â”œâ”€ æœªæŒ‡å®šç›®å½•æˆ–æ–‡ä»¶ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°è®­ç»ƒç›®å½•...")
            latest_dir = PathScanner.get_latest_run_directory("run_\*", root_dir)
            
            if not latest_dir:
                logger.info("  â””â”€ âŒ æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒç›®å½•ï¼ˆéœ€ä»¥'run_'å¼€å¤´ï¼‰")
                return []
                
            target_files = PathScanner.find_metric_files(latest_dir)
            logger.info(f"  â””â”€ è‡ªåŠ¨é€‰æ‹©æœ€æ–°ç›®å½•: {latest_dir} (æ‰¾åˆ° {len(target_files)} ä¸ªæŒ‡æ ‡æ–‡ä»¶)")

        if not target_files:
            logger.info("  â””â”€ âŒ æœªæ‰¾åˆ°ä»»ä½•æŒ‡æ ‡æ–‡ä»¶")
            return []

        # 2. æ‰¹é‡è§£ææŒ‡æ ‡ï¼ˆé€šè¿‡è§£æå™¨æ³¨å†Œä¸­å¿ƒï¼‰
        metric_datas = []
        for file in target_files:
            data = MetricParserRegistry.parse_file(file)
            if data:
                metric_datas.append(data)

        if not metric_datas:
            logger.info("  â””â”€ âŒ æ²¡æœ‰è§£ææˆåŠŸçš„æŒ‡æ ‡æ•°æ®")
            return []

        # 3. æ‰¹é‡å¯è§†åŒ–ï¼ˆé€šè¿‡å¯è§†åŒ–å™¨æ³¨å†Œä¸­å¿ƒï¼‰
        results = []
        for data in metric_datas:
            logger.info(f"  â”œâ”€ å¯è§†åŒ–æŒ‡æ ‡: {data.metric_type} (æ¥æº: {os.path.basename(data.source_path)})")
            vis_result = VisualizerRegistry.draw(data)
            if vis_result:
                results.append(vis_result)

        logger.info(f"  â””â”€ âœ… å®Œæˆå¯è§†åŒ–ï¼Œå…±å¤„ç† {len(results)} ä¸ªæŒ‡æ ‡")
        return results


# ==================================================
# ä¸»å‡½æ•°å…¥å£
# ==================================================
def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="æ¨¡å‹åˆ†æå·¥å…·ï¼šæ±‡æ€»è®­ç»ƒç»“æœã€æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ã€å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡"
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="summarize",
        choices=["summarize", "compare", "latest", "analyze"],
        help="è¿è¡Œæ¨¡å¼: "
        "summarize(æ±‡æ€»è®­ç»ƒç›®å½•), "
        "compare(æŒ‰ç›®å½•æŸ¥æ‰¾æ¨¡å‹å¹¶æ¯”è¾ƒ), "
        "latest(æ¯”è¾ƒæœ€æ–°Nä¸ªç›®å½•çš„æœ€ä½³æ¨¡å‹), "
        "analyze(å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡)",
    )
    parser.add_argument(
        "--pattern",
        type=str,
        default="runs/run_*",
        help="ç›®å½•åŒ¹é…æ¨¡å¼ï¼ˆsummarize/compare/latestæ¨¡å¼å‡ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=10,
        help="æ˜¾ç¤ºçš„ç»“æœæ•°é‡ï¼ˆsummarize/compareæ¨¡å¼ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--num-latest", type=int, default=5, help="å–æœ€æ–°çš„ç›®å½•æ•°é‡ï¼ˆä»…latestæ¨¡å¼ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--root-dir", type=str, default=".", help="æ ¹æœç´¢ç›®å½•ï¼ˆæ‰€æœ‰æ¨¡å¼å‡ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--run-dir", type=str, default=None, help="è®­ç»ƒç›®å½•è·¯å¾„ï¼ˆä»…analyzeæ¨¡å¼ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--metrics-path",
        type=str,
        default=None,
        help="æŒ‡æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆä»…analyzeæ¨¡å¼ç”Ÿæ•ˆï¼‰",
    )

    args = parser.parse_args()

    # æŒ‰æ¨¡å¼è°ƒç”¨ModelAnalysisServiceçš„å¯¹åº”é™æ€æ–¹æ³•
    if args.mode == "summarize":
        ModelAnalysisService.summarize_runs(args.pattern, args.top_n, args.root_dir)
    elif args.mode == "compare":
        ModelAnalysisService.compare_models_by_dir(
            dir_pattern=args.pattern, root_dir=args.root_dir, top_n=args.top_n
        )
    elif args.mode == "latest":
        ModelAnalysisService.compare_latest_models(
            pattern=args.pattern, num_latest=args.num_latest, root_dir=args.root_dir
        )
    elif args.mode == "analyze":
        ModelAnalysisService.visualize_training_metrics(
            run_dir=args.run_dir, metrics_path=args.metrics_path, root_dir=args.root_dir
        )


if __name__ == "__main__":
    main()