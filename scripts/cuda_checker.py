#!/usr/bin/env python3
"""
CUDAå¯ç”¨æ€§æ£€æŸ¥å™¨

åŠŸèƒ½: æ£€æŸ¥ç³»ç»Ÿä¸­çš„CUDAæ”¯æŒæƒ…å†µã€PyTorché…ç½®ä»¥åŠGPUè®¾å¤‡ä¿¡æ¯ï¼Œ
å¸®åŠ©ç”¨æˆ·è¯Šæ–­æ·±åº¦å­¦ä¹ ç¯å¢ƒçš„GPUåŠ é€Ÿå¯è¡Œæ€§ã€‚

å‚æ•°:
  -v, --verbose   å¯ç”¨è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼Œæ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯
  -h, --help      æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯å¹¶é€€å‡º

ç”¨æ³•ç¤ºä¾‹:
  åŸºæœ¬æ£€æŸ¥:
    python cuda_checker.py
  
  è¯¦ç»†æ£€æŸ¥:
    python cuda_checker.py -v
"""

import torch
import sys
import datetime
import subprocess
import platform
import argparse


class CudaChecker:
    """
    CUDAå¯ç”¨æ€§æ£€æŸ¥å™¨ï¼Œæä¾›ä¼˜é›…çš„æ¥å£æ¥æ£€æŸ¥å’Œæ˜¾ç¤ºCUDAç›¸å…³ä¿¡æ¯
    """

    def __init__(self, verbose=False):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨ï¼Œè®°å½•å¼€å§‹æ—¶é—´"""
        self.verbose = verbose
        self.start_time = datetime.datetime.now()
        self.end_time = None
        self.cuda_available = False
        self.device_count = 0

    def _print_header(self):
        """æ‰“å°æ£€æŸ¥å¤´éƒ¨ä¿¡æ¯"""
        print("=" * 60)
        print(
            f"ğŸ” CUDA å¯ç”¨æ€§æ£€æŸ¥ - å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )
        if self.verbose:
            print("ğŸ“ è¯¦ç»†è¾“å‡ºæ¨¡å¼å·²å¯ç”¨")
        print("=" * 60)

    def _print_footer(self):
        """æ‰“å°æ£€æŸ¥å°¾éƒ¨ä¿¡æ¯"""
        self.end_time = datetime.datetime.now()
        elapsed_time = (self.end_time - self.start_time).total_seconds()

        print("=" * 60)
        print(
            f"âœ… CUDA å¯ç”¨æ€§æ£€æŸ¥å®Œæˆ - ç»“æŸæ—¶é—´: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )
        print(f"â±ï¸  æ£€æŸ¥è€—æ—¶: {elapsed_time:.2f} ç§’")
        print("=" * 60)

    def check_system_info(self):
        """æ£€æŸ¥å¹¶æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print(f"\nğŸ–¥ï¸  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
        print(f"ğŸ–¥ï¸  ç³»ç»Ÿæ¶æ„: {platform.machine()}")
        print(f"âœ… Python ç‰ˆæœ¬: {sys.version.split(' ')[0]}")
        
        if self.verbose:
            print(f"ğŸ“Š è¯¦ç»†ç³»ç»Ÿä¿¡æ¯: {platform.platform()}")
            print(f"ğŸ“Š Python å®ç°: {platform.python_implementation()}")

    def check_pytorch(self):
        """æ£€æŸ¥PyTorchå®‰è£…çŠ¶æ€"""
        try:
            print(f"\nğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}")

            # æ£€æŸ¥PyTorchçš„CUDAç‰ˆæœ¬
            torch_cuda_version = (
                torch.version.cuda if hasattr(torch.version, "cuda") else "N/A"
            )
            print(f"ğŸ”— PyTorch å†…ç½®CUDAç‰ˆæœ¬: {torch_cuda_version}")

            # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
            self.cuda_available = torch.cuda.is_available()
            print(f"ğŸ“Š CUDA å¯ç”¨çŠ¶æ€: {self.cuda_available}")

            # å°è¯•è·å–è®¾å¤‡æ•°é‡ä¿¡æ¯
            self._get_device_count()
            return True
        except ImportError:
            print("âŒ é”™è¯¯: æœªå®‰è£…PyTorchåº“")
            print("ğŸ’¡ æç¤º: è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
            print(
                "  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
            )
            return False
        except Exception as e:
            print(f"âŒ æ£€æŸ¥PyTorchè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            if self.verbose:
                import traceback
                traceback.print_exc()
            return False

    def _get_device_count(self):
        """è·å–GPUè®¾å¤‡æ•°é‡"""
        try:
            self.device_count = (
                torch.cuda.device_count() if hasattr(torch.cuda, "device_count") else 0
            )
            print(f"ğŸ’» æ£€æµ‹åˆ°çš„GPUè®¾å¤‡æ•°é‡: {self.device_count}")
        except:
            print("ğŸ’» æ— æ³•è·å–GPUè®¾å¤‡æ•°é‡")
            self.device_count = 0

    def check_gpu_devices(self):
        """æ£€æŸ¥å¹¶æ˜¾ç¤ºæ‰€æœ‰GPUè®¾å¤‡çš„è¯¦ç»†ä¿¡æ¯"""
        if not self.cuda_available or self.device_count == 0:
            return

        # è·å–CUDAç‰ˆæœ¬ä¿¡æ¯
        cuda_version = torch.version.cuda
        print(f"\nğŸ¯ CUDA ç‰ˆæœ¬: {cuda_version}")
        print(f"ğŸ¯ cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"ğŸ’» GPU è®¾å¤‡æ•°é‡: {self.device_count}")

        # éå†æ‰€æœ‰GPUè®¾å¤‡å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        for i in range(self.device_count):
            self._show_gpu_details(i)
            self._test_gpu_computation(i)

        # æ˜¾ç¤ºå½“å‰é»˜è®¤GPUè®¾å¤‡
        current_device = torch.cuda.current_device()
        print(
            f"\nğŸ¯ å½“å‰é»˜è®¤GPUè®¾å¤‡: #{current_device} ({torch.cuda.get_device_name(current_device)})"
        )

    def _show_gpu_details(self, device_index):
        """æ˜¾ç¤ºæŒ‡å®šGPUè®¾å¤‡çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            device_name = torch.cuda.get_device_name(device_index)
            device_properties = torch.cuda.get_device_properties(device_index)

            print(f"\n--- GPU è®¾å¤‡ #{device_index+1} ---")
            print(f"è®¾å¤‡åç§°: {device_name}")
            print(f"è®¡ç®—èƒ½åŠ›: {device_properties.major}.{device_properties.minor}")
            print(f"æ€»å†…å­˜: {device_properties.total_memory / 1024**3:.2f} GB")
            print(f"å¤šå¤„ç†å™¨æ•°é‡: {device_properties.multi_processor_count}")
            
            if self.verbose:
                # å®‰å…¨åœ°è®¿é—®å¯èƒ½ä¸å­˜åœ¨çš„å±æ€§
                if hasattr(device_properties, 'memory_clock_rate'):
                    print(f"å†…å­˜æ—¶é’Ÿé¢‘ç‡: {device_properties.memory_clock_rate / 1e6:.2f} MHz")
                else:
                    print("å†…å­˜æ—¶é’Ÿé¢‘ç‡: ä¸æ”¯æŒçš„å±æ€§")
                    
                if hasattr(device_properties, 'clock_rate'):
                    print(f"æ ¸å¿ƒæ—¶é’Ÿé¢‘ç‡: {device_properties.clock_rate / 1e6:.2f} MHz")
                else:
                    print("æ ¸å¿ƒæ—¶é’Ÿé¢‘ç‡: ä¸æ”¯æŒçš„å±æ€§")
                    
                if hasattr(device_properties, 'max_threads_per_block'):
                    print(f"æ”¯æŒçš„æœ€å¤§çº¿ç¨‹æ•°: {device_properties.max_threads_per_block}")
                else:
                    print("æ”¯æŒçš„æœ€å¤§çº¿ç¨‹æ•°: ä¸æ”¯æŒçš„å±æ€§")
        except Exception as e:
            print(f"âŒ è·å–GPU #{device_index+1} ä¿¡æ¯å¤±è´¥: {str(e)}")
            if self.verbose:
                import traceback
                traceback.print_exc()

    def _test_gpu_computation(self, device_index):
        """åœ¨æŒ‡å®šGPUä¸Šæ‰§è¡Œç®€å•è®¡ç®—æµ‹è¯•"""
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¼ é‡å¹¶ç§»è‡³GPU
            test_tensor = torch.tensor([1.0, 2.0, 3.0]).to(f"cuda:{device_index}")
            result_tensor = test_tensor * 2
            
            if self.verbose:
                print(f"ğŸ“Š è®¡ç®—æµ‹è¯•è¾“å…¥: {test_tensor.cpu().numpy()}")
                print(f"ğŸ“Š è®¡ç®—æµ‹è¯•è¾“å‡º: {result_tensor.cpu().numpy()}")
                
            print(f"âœ… GPU #{device_index+1} è®¡ç®—æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âŒ GPU #{device_index+1} è®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
            if self.verbose:
                import traceback
                traceback.print_exc()

    def check_nvidia_smi(self):
        """ä½¿ç”¨nvidia-smiå‘½ä»¤æ£€æŸ¥NVIDIAè®¾å¤‡çŠ¶æ€"""
        print("\nğŸ”§ æ‰§è¡Œç³»ç»Ÿå‘½ä»¤æ£€æŸ¥NVIDIAè®¾å¤‡å’ŒCUDAçŠ¶æ€...")
        try:
            # æ£€æŸ¥nvidia-smiå‘½ä»¤æ˜¯å¦å­˜åœ¨
            if platform.system() == "Windows":
                nvidia_smi_output = subprocess.check_output(
                    ["nvidia-smi"], shell=True, stderr=subprocess.STDOUT
                ).decode("utf-8")
            else:
                # Linux/Mac
                nvidia_smi_output = subprocess.check_output(
                    ["nvidia-smi"], stderr=subprocess.STDOUT
                ).decode("utf-8")

            print("âœ… ç³»ç»Ÿä¸­æ£€æµ‹åˆ°NVIDIAè®¾å¤‡:")
            # åªæ˜¾ç¤ºå‰å‡ è¡Œå…³é”®ä¿¡æ¯
            lines = nvidia_smi_output.split("\n")
            for line in lines[:7]:
                if line.strip():
                    print(f"   {line.strip()}")
            
            if self.verbose:
                print("\nğŸ“ å®Œæ•´nvidia-smiè¾“å‡º:")
                for line in lines:
                    if line.strip():
                        print(f"   {line.strip()}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•æ‰§è¡Œnvidia-smiå‘½ä»¤: {str(e)}")
            if self.verbose:
                import traceback
                traceback.print_exc()

    def show_cuda_unavailable_reasons(self):
        """æ˜¾ç¤ºCUDAä¸å¯ç”¨çš„å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ"""
        print("\nâ“ CUDA ä¸å¯ç”¨çš„å¯èƒ½åŸå› :")
        print("1. æ‚¨çš„æœºå™¨æ²¡æœ‰NVIDIA GPU")
        print("2. æœªå®‰è£…NVIDIA CUDAé©±åŠ¨ç¨‹åº")
        print("3. å®‰è£…çš„PyTorchç‰ˆæœ¬ä¸æ”¯æŒCUDA")
        print("4. CUDAé©±åŠ¨ç¨‹åºç‰ˆæœ¬ä¸PyTorchè¦æ±‚ä¸å…¼å®¹")
        print("5. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜")

        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("- å¦‚æœç³»ç»Ÿæœ‰NVIDIA GPUä½†PyTorchæ£€æµ‹ä¸åˆ°ï¼Œè¯·å®‰è£…æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬")
        print("- åœ¨Windowsä¸Šï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
        print(
            "  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
        )
        print("- åœ¨Linuxä¸Šï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
        print(
            "  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
        )
        print("- è¯·æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„PyTorchç‰ˆæœ¬")

    def run_check(self):
        """è¿è¡Œå®Œæ•´çš„CUDAå¯ç”¨æ€§æ£€æŸ¥"""
        try:
            self._print_header()
            self.check_system_info()

            pytorch_available = self.check_pytorch()
            if pytorch_available:
                if self.cuda_available:
                    self.check_gpu_devices()
                else:
                    self.check_nvidia_smi()
                    self.show_cuda_unavailable_reasons()
        except Exception as e:
            print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {str(e)}")
            if self.verbose:
                import traceback
                traceback.print_exc()
        finally:
            self._print_footer()


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="CUDAå¯ç”¨æ€§æ£€æŸ¥å™¨ï¼Œç”¨äºè¯Šæ–­ç³»ç»Ÿä¸­çš„CUDAæ”¯æŒæƒ…å†µå’ŒGPUè®¾å¤‡ä¿¡æ¯ã€‚",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""ä½¿ç”¨ç¤ºä¾‹:
  åŸºæœ¬æ£€æŸ¥:
    python cuda_checker.py
  
  è¯¦ç»†æ£€æŸ¥:
    python cuda_checker.py -v"""
    )
    
    parser.add_argument(
        "-v", "--verbose", 
        action="store_true", 
        help="å¯ç”¨è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼Œæ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡ŒCUDAæ£€æŸ¥æµç¨‹"""
    args = parse_arguments()
    cuda_checker = CudaChecker(verbose=args.verbose)
    cuda_checker.run_check()


if __name__ == "__main__":
    main()
    