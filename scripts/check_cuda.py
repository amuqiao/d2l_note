import torch
import sys
import datetime
import subprocess
import platform


class CudaChecker:
    """
    CUDAå¯ç”¨æ€§æ£€æŸ¥å™¨ï¼Œæä¾›ä¼˜é›…çš„æ¥å£æ¥æ£€æŸ¥å’Œæ˜¾ç¤ºCUDAç›¸å…³ä¿¡æ¯
    """

    def __init__(self):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨ï¼Œè®°å½•å¼€å§‹æ—¶é—´"""
        self.start_time = datetime.datetime.now()
        self.end_time = None
        self.cuda_available = False
        self.device_count = 0

    def _print_header(self):
        """æ‰“å°æ£€æŸ¥å¤´éƒ¨ä¿¡æ¯"""
        print("=" * 60)
        print(
            f"ğŸ” CUDA å¯ç”¨æ€§æ£€æŸ¥ - å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )
        print("=" * 60)

    def _print_footer(self):
        """æ‰“å°æ£€æŸ¥å°¾éƒ¨ä¿¡æ¯"""
        self.end_time = datetime.datetime.now()
        elapsed_time = (self.end_time - self.start_time).total_seconds()

        print("=" * 60)
        print(
            f"âœ… CUDA å¯ç”¨æ€§æ£€æŸ¥å®Œæˆ - ç»“æŸæ—¶é—´: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )
        print(f"â±ï¸  æ£€æŸ¥è€—æ—¶: {elapsed_time:.2f} ç§’")
        print("=" * 60)

    def check_system_info(self):
        """æ£€æŸ¥å¹¶æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print(f"ğŸ–¥ï¸  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
        print(f"âœ… Python ç‰ˆæœ¬: {sys.version.split(' ')[0]}")

    def check_pytorch(self):
        """æ£€æŸ¥PyTorchå®‰è£…çŠ¶æ€"""
        try:
            print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")

            # æ£€æŸ¥PyTorchçš„CUDAç‰ˆæœ¬
            torch_cuda_version = (
                torch.version.cuda if hasattr(torch.version, "cuda") else "N/A"
            )
            print(f"ğŸ”— PyTorch å†…ç½®CUDAç‰ˆæœ¬: {torch_cuda_version}")

            # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
            self.cuda_available = torch.cuda.is_available()
            print(f"ğŸ“Š CUDA å¯ç”¨çŠ¶æ€: {self.cuda_available}")

            # å°è¯•è·å–è®¾å¤‡æ•°é‡ä¿¡æ¯
            self._get_device_count()
            return True
        except ImportError:
            print("âŒ é”™è¯¯: æœªå®‰è£…PyTorchåº“")
            print("ğŸ’¡ æç¤º: è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
            print(
                "  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
            )
            return False
        except Exception as e:
            print(f"âŒ æ£€æŸ¥PyTorchè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback

            traceback.print_exc()
            return False

    def _get_device_count(self):
        """è·å–GPUè®¾å¤‡æ•°é‡"""
        try:
            self.device_count = (
                torch.cuda.device_count() if hasattr(torch.cuda, "device_count") else 0
            )
            print(f"ğŸ’» æ£€æµ‹åˆ°çš„GPUè®¾å¤‡æ•°é‡: {self.device_count}")
        except:
            print("ğŸ’» æ— æ³•è·å–GPUè®¾å¤‡æ•°é‡")
            self.device_count = 0

    def check_gpu_devices(self):
        """æ£€æŸ¥å¹¶æ˜¾ç¤ºæ‰€æœ‰GPUè®¾å¤‡çš„è¯¦ç»†ä¿¡æ¯"""
        if not self.cuda_available or self.device_count == 0:
            return

        # è·å–CUDAç‰ˆæœ¬ä¿¡æ¯
        cuda_version = torch.version.cuda
        print(f"ğŸ¯ CUDA ç‰ˆæœ¬: {cuda_version}")

        print(f"ğŸ’» GPU è®¾å¤‡æ•°é‡: {self.device_count}")

        # éå†æ‰€æœ‰GPUè®¾å¤‡å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        for i in range(self.device_count):
            self._show_gpu_details(i)
            self._test_gpu_computation(i)

        # æ˜¾ç¤ºå½“å‰é»˜è®¤GPUè®¾å¤‡
        current_device = torch.cuda.current_device()
        print(
            f"\nğŸ¯ å½“å‰é»˜è®¤GPUè®¾å¤‡: #{current_device} ({torch.cuda.get_device_name(current_device)})"
        )

    def _show_gpu_details(self, device_index):
        """æ˜¾ç¤ºæŒ‡å®šGPUè®¾å¤‡çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            device_name = torch.cuda.get_device_name(device_index)
            device_properties = torch.cuda.get_device_properties(device_index)

            print(f"\n--- GPU è®¾å¤‡ #{device_index+1} ---")
            print(f"è®¾å¤‡åç§°: {device_name}")
            print(f"è®¡ç®—èƒ½åŠ›: {device_properties.major}.{device_properties.minor}")
            print(f"æ€»å†…å­˜: {device_properties.total_memory / 1024**3:.2f} GB")
            print(f"å¤šå¤„ç†å™¨æ•°é‡: {device_properties.multi_processor_count}")
        except Exception as e:
            print(f"âŒ è·å–GPU #{device_index+1} ä¿¡æ¯å¤±è´¥: {str(e)}")

    def _test_gpu_computation(self, device_index):
        """åœ¨æŒ‡å®šGPUä¸Šæ‰§è¡Œç®€å•è®¡ç®—æµ‹è¯•"""
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¼ é‡å¹¶ç§»è‡³GPU
            test_tensor = torch.tensor([1.0, 2.0, 3.0]).to(f"cuda:{device_index}")
            result_tensor = test_tensor * 2
            print(f"âœ… GPU #{device_index+1} è®¡ç®—æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âŒ GPU #{device_index+1} è®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")

    def check_nvidia_smi(self):
        """ä½¿ç”¨nvidia-smiå‘½ä»¤æ£€æŸ¥NVIDIAè®¾å¤‡çŠ¶æ€"""
        print("\nğŸ”§ æ‰§è¡Œç³»ç»Ÿå‘½ä»¤æ£€æŸ¥NVIDIAè®¾å¤‡å’ŒCUDAçŠ¶æ€...")
        try:
            # æ£€æŸ¥nvidia-smiå‘½ä»¤æ˜¯å¦å­˜åœ¨
            if platform.system() == "Windows":
                nvidia_smi_output = subprocess.check_output(
                    ["nvidia-smi"], shell=True, stderr=subprocess.STDOUT
                ).decode("utf-8")
            else:
                # Linux/Mac
                nvidia_smi_output = subprocess.check_output(
                    ["nvidia-smi"], stderr=subprocess.STDOUT
                ).decode("utf-8")

            print("âœ… ç³»ç»Ÿä¸­æ£€æµ‹åˆ°NVIDIAè®¾å¤‡:")
            # åªæ˜¾ç¤ºå‰å‡ è¡Œå…³é”®ä¿¡æ¯
            for line in nvidia_smi_output.split("\n")[:7]:
                if line.strip():
                    print(f"   {line.strip()}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•æ‰§è¡Œnvidia-smiå‘½ä»¤: {str(e)}")

    def show_cuda_unavailable_reasons(self):
        """æ˜¾ç¤ºCUDAä¸å¯ç”¨çš„å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ"""
        print("\nâ“ CUDA ä¸å¯ç”¨çš„å¯èƒ½åŸå› :")
        print("1. æ‚¨çš„æœºå™¨æ²¡æœ‰NVIDIA GPU")
        print("2. æœªå®‰è£…NVIDIA CUDAé©±åŠ¨ç¨‹åº")
        print("3. å®‰è£…çš„PyTorchç‰ˆæœ¬ä¸æ”¯æŒCUDA")
        print("4. CUDAé©±åŠ¨ç¨‹åºç‰ˆæœ¬ä¸PyTorchè¦æ±‚ä¸å…¼å®¹")
        print("5. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜")

        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("- å¦‚æœç³»ç»Ÿæœ‰NVIDIA GPUä½†PyTorchæ£€æµ‹ä¸åˆ°ï¼Œè¯·å®‰è£…æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬")
        print("- åœ¨Windowsä¸Šï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
        print(
            "  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
        )
        print("- åœ¨Linuxä¸Šï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
        print(
            "  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
        )
        print("- è¯·æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„PyTorchç‰ˆæœ¬")

    def run_check(self):
        """è¿è¡Œå®Œæ•´çš„CUDAå¯ç”¨æ€§æ£€æŸ¥"""
        try:
            self._print_header()
            self.check_system_info()

            pytorch_available = self.check_pytorch()
            if pytorch_available:
                if self.cuda_available:
                    self.check_gpu_devices()
                else:
                    self.check_nvidia_smi()
                    self.show_cuda_unavailable_reasons()
        except Exception as e:
            print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {str(e)}")
            import traceback

            traceback.print_exc()
        finally:
            self._print_footer()


if __name__ == "__main__":
    cuda_checker = CudaChecker()
    cuda_checker.run_check()
